{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"aspp.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"ZWVY0iGOFL_G"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ASPP(nn.Module):\n","    def __init__(self, num_classes):\n","        super(ASPP, self).__init__()\n","\n","        self.conv_1x1_1 = nn.Conv2d(512, 256, kernel_size=1)\n","        self.bn_conv_1x1_1 = nn.BatchNorm2d(256)\n","\n","        self.conv_3x3_1 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=6, dilation=6)\n","        self.bn_conv_3x3_1 = nn.BatchNorm2d(256)\n","\n","        self.conv_3x3_2 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=12, dilation=12)\n","        self.bn_conv_3x3_2 = nn.BatchNorm2d(256)\n","\n","        self.conv_3x3_3 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=18, dilation=18)\n","        self.bn_conv_3x3_3 = nn.BatchNorm2d(256)\n","\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","\n","        self.conv_1x1_2 = nn.Conv2d(512, 256, kernel_size=1)\n","        self.bn_conv_1x1_2 = nn.BatchNorm2d(256)\n","\n","        self.conv_1x1_3 = nn.Conv2d(1280, 256, kernel_size=1) # (1280 = 5*256)\n","        self.bn_conv_1x1_3 = nn.BatchNorm2d(256)\n","\n","        self.conv_1x1_4 = nn.Conv2d(256, num_classes, kernel_size=1)\n","\n","    def forward(self, feature_map):\n","        # (feature_map has shape (batch_size, 512, h/16, w/16)) (assuming self.resnet is ResNet18_OS16 or ResNet34_OS16. If self.resnet instead is ResNet18_OS8 or ResNet34_OS8, it will be (batch_size, 512, h/8, w/8))\n","\n","        feature_map_h = feature_map.size()[2] # (== h/16)\n","        feature_map_w = feature_map.size()[3] # (== w/16)\n","        #print('input.shape....', feature_map.shape)\n","        out_1x1 = F.relu(self.bn_conv_1x1_1(self.conv_1x1_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n","        out_3x3_1 = F.relu(self.bn_conv_3x3_1(self.conv_3x3_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n","        out_3x3_2 = F.relu(self.bn_conv_3x3_2(self.conv_3x3_2(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n","        out_3x3_3 = F.relu(self.bn_conv_3x3_3(self.conv_3x3_3(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n","\n","        out_img = self.avg_pool(feature_map) # (shape: (batch_size, 512, 1, 1))\n","        out_img = F.relu(self.bn_conv_1x1_2(self.conv_1x1_2(out_img))) # (shape: (batch_size, 256, 1, 1))\n","        out_img = F.upsample(out_img, size=(feature_map_h, feature_map_w), mode=\"bilinear\") # (shape: (batch_size, 256, h/16, w/16))\n","\n","        out = torch.cat([out_1x1, out_3x3_1, out_3x3_2, out_3x3_3, out_img], 1) # (shape: (batch_size, 1280, h/16, w/16))\n","        #print('out.shape....', out.shape)\n","        #~~~\n","        bias_fork = out  # this will be the fork location for the bias removal head\n","        #~~~\n","        out = F.relu(self.bn_conv_1x1_3(self.conv_1x1_3(out))) # (shape: (batch_size, 256, h/16, w/16))\n","        out = self.conv_1x1_4(out) # (shape: (batch_size, num_classes, h/16, w/16))\n","        #print('out.shape at the end...', out.shape)\n","\n","        return out , bias_fork\n","\n","class ASPP_Bottleneck(nn.Module):\n","    def __init__(self, num_classes):\n","        super(ASPP_Bottleneck, self).__init__()\n","\n","        self.conv_1x1_1 = nn.Conv2d(4*512, 256, kernel_size=1)\n","        self.bn_conv_1x1_1 = nn.BatchNorm2d(256)\n","\n","        self.conv_3x3_1 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=6, dilation=6)\n","        self.bn_conv_3x3_1 = nn.BatchNorm2d(256)\n","\n","        self.conv_3x3_2 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=12, dilation=12)\n","        self.bn_conv_3x3_2 = nn.BatchNorm2d(256)\n","\n","        self.conv_3x3_3 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=18, dilation=18)\n","        self.bn_conv_3x3_3 = nn.BatchNorm2d(256)\n","\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","\n","        self.conv_1x1_2 = nn.Conv2d(4*512, 256, kernel_size=1)\n","        self.bn_conv_1x1_2 = nn.BatchNorm2d(256)\n","\n","        self.conv_1x1_3 = nn.Conv2d(1280, 256, kernel_size=1) # (1280 = 5*256)\n","        self.bn_conv_1x1_3 = nn.BatchNorm2d(256)\n","\n","        self.conv_1x1_4 = nn.Conv2d(256, num_classes, kernel_size=1)\n","\n","    def forward(self, feature_map):\n","        # (feature_map has shape (batch_size, 4*512, h/16, w/16))\n","\n","        feature_map_h = feature_map.size()[2] # (== h/16)\n","        feature_map_w = feature_map.size()[3] # (== w/16)\n","\n","        out_1x1 = F.relu(self.bn_conv_1x1_1(self.conv_1x1_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n","        out_3x3_1 = F.relu(self.bn_conv_3x3_1(self.conv_3x3_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n","        out_3x3_2 = F.relu(self.bn_conv_3x3_2(self.conv_3x3_2(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n","        out_3x3_3 = F.relu(self.bn_conv_3x3_3(self.conv_3x3_3(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n","\n","        out_img = self.avg_pool(feature_map) # (shape: (batch_size, 512, 1, 1))\n","        out_img = F.relu(self.bn_conv_1x1_2(self.conv_1x1_2(out_img))) # (shape: (batch_size, 256, 1, 1))\n","        out_img = F.upsample(out_img, size=(feature_map_h, feature_map_w), mode=\"bilinear\") # (shape: (batch_size, 256, h/16, w/16))\n","        \n","        out = torch.cat([out_1x1, out_3x3_1, out_3x3_2, out_3x3_3, out_img], 1) # (shape: (batch_size, 1280, h/16, w/16))\n","        #print('out....', out)\n","        #~~~\n","        bias_fork = out # (shape: b, 1280, h/16, w/16)\n","        #~~~\n","        out = F.relu(self.bn_conv_1x1_3(self.conv_1x1_3(out))) # (shape: (batch_size, 256, h/16, w/16))\n","        out = self.conv_1x1_4(out) # (shape: (batch_size, num_classes, h/16, w/16))\n","        #print('out at the end...', out)\n","\n","        return out, bias_fork"],"execution_count":null,"outputs":[]}]}