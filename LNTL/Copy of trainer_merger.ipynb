{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"trainer_merger.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPXS568gNGsqKHGd0EUhHYE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4q5B9A7uabXu","executionInfo":{"status":"ok","timestamp":1627044671853,"user_tz":-60,"elapsed":19434,"user":{"displayName":"Jack Stelling","photoUrl":"","userId":"10755116166180930038"}},"outputId":"9ef7146d-c126-4ea5-9de6-4c8c0bdd4512"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lrjM8xv4a36f","executionInfo":{"status":"ok","timestamp":1627044757720,"user_tz":-60,"elapsed":216,"user":{"displayName":"Jack Stelling","photoUrl":"","userId":"10755116166180930038"}},"outputId":"4d14a532-4437-467f-8cf5-fc05bf951d58"},"source":["import os\n","os.chdir('/content/drive/MyDrive/BiasMitigation/LNTL')\n","print('Changed the local path to....', os.getcwd())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Changed the local path to.... /content/drive/MyDrive/BiasMitigation/LNTL\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7Xoqp4k5_cmt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627044891272,"user_tz":-60,"elapsed":7966,"user":{"displayName":"Jack Stelling","photoUrl":"","userId":"10755116166180930038"}},"outputId":"df18794f-86e7-4f27-ae79-cb64e2bb3768"},"source":["# Local scripts\n","! pip install import_ipynb\n","\n","import import_ipynb\n","\n","from models.Deeplab import deeplabv3\n","from models.SegNet import segnet\n","from models import biashead\n","from utils.utils_LNTL import logger_setting, Timer\n","# Python\n","import time\n","import os\n","import math\n","import numpy as np\n","import numpy as np\n","import pickle\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","import matplotlib.pyplot as plt\n","# Torch\n","import torch\n","from torch import nn\n","from torch import optim\n","from torch.autograd import Variable\n","# Colab \n","from google.colab.patches import cv2_imshow\n","\n","\n","class GradReverse(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, x):\n","        return x.view_as(x)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        return grad_output.neg() * 0.1\n","\n","def grad_reverse(x):\n","    return GradReverse.apply(x)\n","\n","\n","\n","class Trainer(object):\n","    def __init__(self, option, path):\n","        self.option = option\n","        self.path = path\n","\n","        self._build_model()\n","        self._set_optimizer()\n","        self.logger = logger_setting(option.exp_name, option.save_dir, path, option.debug)\n","\n","    def _build_model(self):\n","        self.n_color_cls = 8    # This is how many colour bins we have used 256/32\n","\n","        # The bias input channels depend on fork placement \n","        if self.option.network_type == 'Deeplab': # CAN WE JUST USE option.network_type HERE BECUASE OF LINE 31\n","            bias_input_channels = 1280  # The number of feature maps after the aspp concat step.\n","        elif self.option.network_type == 'Segnet':\n","            bias_input_channels = 64\n","\n","        #self.net = deeplabv3.DeepLabV3(model_id=self.option.exp_name, project_dir = self.option.save_dir) changed from this becuase i changed the arguements in DeepLabV3\n","        self.net = deeplabv3.DeepLabV3() # Doesnt need number of classes as its hard coded at the top of the deeplab class\n","        # The path might have to be changed since we have a directory models with the models inside. Rather than a script with the models inside as this is being called.\n","        # Also the deeplab class calls Aspp and Resnet so check paths in those scripts. \n","        self.pred_net_r = biashead.BiasPredictor(input_ch=bias_input_channels, num_classes=self.n_color_cls)\n","        self.pred_net_g = biashead.BiasPredictor(input_ch=bias_input_channels, num_classes=self.n_color_cls)\n","        self.pred_net_b = biashead.BiasPredictor(input_ch=bias_input_channels, num_classes=self.n_color_cls)\n","\n","\n","        with open( self.option.meta_dir + \"/class_weights.pkl\", \"rb\") as file: # (needed for python3)\n","            class_weights = np.array(pickle.load(file))\n","        class_weights = torch.from_numpy(class_weights)\n","        class_weights = Variable(class_weights.type(torch.FloatTensor)).cuda()\n","\n","\n","        self.loss = nn.CrossEntropyLoss(weight = class_weights)  # must add weights to this one to perform cross entropy relatively (a traffic light is as important as a building)\n","        self.color_loss = nn.CrossEntropyLoss()\n","\n","\n","        if self.option.cuda:\n","            self.net.cuda()\n","            self.pred_net_r.cuda()\n","            self.pred_net_g.cuda()\n","            self.pred_net_b.cuda()\n","            self.loss.cuda()\n","            self.color_loss.cuda()\n","\n","    def _set_optimizer(self): # worth adding in ADAM here?\n","        self.optim = optim.SGD(filter(lambda p: p.requires_grad, self.net.parameters()), lr=self.option.lr, momentum=self.option.momentum, weight_decay=self.option.weight_decay)\n","        self.optim_r = optim.SGD(self.pred_net_r.parameters(), lr=self.option.lr, momentum=self.option.momentum, weight_decay=self.option.weight_decay)\n","        self.optim_g = optim.SGD(self.pred_net_g.parameters(), lr=self.option.lr, momentum=self.option.momentum, weight_decay=self.option.weight_decay)\n","        self.optim_b = optim.SGD(self.pred_net_b.parameters(), lr=self.option.lr, momentum=self.option.momentum, weight_decay=self.option.weight_decay)\n","\n","        #TODO: last_epoch should be the last step of loaded model\n","        lr_lambda = lambda step: self.option.lr_decay_rate ** (step // self.option.lr_decay_period)\n","        self.scheduler = optim.lr_scheduler.LambdaLR(self.optim, lr_lambda=lr_lambda, last_epoch=-1)\n","        self.scheduler_r = optim.lr_scheduler.LambdaLR(self.optim_r, lr_lambda=lr_lambda, last_epoch=-1)\n","        self.scheduler_g = optim.lr_scheduler.LambdaLR(self.optim_g, lr_lambda=lr_lambda, last_epoch=-1)\n","        self.scheduler_b = optim.lr_scheduler.LambdaLR(self.optim_b, lr_lambda=lr_lambda, last_epoch=-1)\n","\n","    @staticmethod\n","    def _weights_init(m):\n","        classname = m.__class__.__name__\n","        if classname.find('Conv') != -1:\n","            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            m.weight.data.normal_(0, math.sqrt(2. / n))\n","        elif classname.find('BatchNorm') != -1:\n","            m.weight.data.fill_(1.0)\n","            m.bias.data.zero_()\n","\n","    def _initialization(self):\n","        self.net.apply(self._weights_init)\n","\n","\n","        if self.option.is_train and self.option.use_pretrain:\n","            if self.option.checkpoint is not None:\n","                self._load_model()\n","            else:\n","                print(\"Pre-trained model not provided\")\n","\n","\n","\n","    def _mode_setting(self, is_train=True):\n","        if is_train:\n","            self.net.train()\n","            self.pred_net_r.train()\n","            self.pred_net_g.train()\n","            self.pred_net_b.train()\n","        else:\n","            self.net.eval()\n","            self.pred_net_r.eval()\n","            self.pred_net_g.eval()\n","            self.pred_net_b.eval()\n","\n","\n","\n","    def _train_step(self, data_loader, step):\n","        _lambda = 0.01    # should we make this an option?\n","\n","        for i, (images, bias_labels, label_imgs) in enumerate(data_loader):\n","            \n","            images = self._get_variable(images)\n","            bias_labels = self._get_variable(bias_labels)\n","            label_imgs = self._get_variable(label_imgs)  # converts to pytorch variable, uses CUDA if GPU available. \n","\n","            # reset gradients \n","            self.optim.zero_grad()\n","            self.optim_r.zero_grad()\n","            self.optim_g.zero_grad()\n","            self.optim_b.zero_grad()\n","            pred_map, softmax, bias_fork = self.net(images)\n","\n","\n","            # predict colors from feat_label. Their prediction should be uniform.\n","            _,pseudo_pred_r = self.pred_net_r(bias_fork)  # outputs x and p(x),  here we get the softmax output\n","            _,pseudo_pred_g = self.pred_net_g(bias_fork)\n","            _,pseudo_pred_b = self.pred_net_b(bias_fork)\n","\n","\n","            # loss for self.net - semantic segmentation\n","            loss_pred = self.loss(pred_map, torch.squeeze(label_imgs)) #do we need squeeze?\n","\n","            loss_pseudo_pred_r = torch.mean(torch.sum(pseudo_pred_r*torch.log(pseudo_pred_r),1))  # manual cross entropy p(x)log(p(x)) \n","            loss_pseudo_pred_g = torch.mean(torch.sum(pseudo_pred_g*torch.log(pseudo_pred_g),1))\n","            loss_pseudo_pred_b = torch.mean(torch.sum(pseudo_pred_b*torch.log(pseudo_pred_b),1))\n","            \n","            \n","            loss_pred_ps_color = (loss_pseudo_pred_r + loss_pseudo_pred_g + loss_pseudo_pred_b) / 3.\n","            loss = loss_pred + loss_pred_ps_color*_lambda\n","            \n","            # DEBUGGING: #####\n","            print('**************************')\n","            print('pseudo_pred_r.... \\n', psuedo_pred_r)\n","            print('pseudo_pred_r.shape =  ', psuedo_pred_r.shape)\n","            print('**************************')\n","            print('loss_pseudo_pred_r  \\n', loss_psuedo_pred_r)\n","            print('loss_pseudo_pred_r.shape = ', loss_psuedo_pred_r.shape)\n","            print('**************************')\n","            print('loss_pred_ps_color \\n', loss_pred_ps_color)\n","            print('loss_pred_ps_color.shape  = ', loss_pred_ps_color.shape)\n","            print('**************************')\n","            print('loss \\n', loss)\n","            ###################\n","            \n","            loss.backward()\n","            self.optim.step()\n","\n","            # Reset gradients for the next stage of training schema\n","            self.optim.zero_grad()\n","            self.optim_r.zero_grad()\n","            self.optim_g.zero_grad()\n","            self.optim_b.zero_grad()\n","\n","            pred_map, softmax, bias_fork = self.net(images)\n","            feat_color = grad_reverse(pred_map)\n","            \n","            pred_r,_ = self.pred_net_r(feat_color)  # outputs x, p(x),  this time we get the layer before the softmax\n","            pred_g,_ = self.pred_net_g(feat_color)\n","            pred_b,_ = self.pred_net_b(feat_color)\n","\n","            # loss for rgb predictors\n","            loss_pred_r = self.color_loss(pred_r, bias_labels[:,0]) # colour_loss() is the cross entropy instance, bias_labels are the ground truths created in dataloaders, [:,0] strips out R component \n","            loss_pred_g = self.color_loss(pred_g, bias_labels[:,1])\n","            loss_pred_b = self.color_loss(pred_b, bias_labels[:,2])\n","\n","            loss_pred_color = loss_pred_r + loss_pred_g + loss_pred_b\n","\n","            loss_pred_color.backward()\n","            self.optim.step()\n","            self.optim_r.step()\n","            self.optim_g.step()\n","            self.optim_b.step()\n","\n","            if i % self.option.log_step == 0:\n","                msg = \"[TRAIN] cls loss : %.6f, rgb : %.6f, MI : %.6f  (epoch %d.%02d)\" \\\n","                       % (loss_pred,loss_pred_color/3.,loss_pred_ps_color,step,int(100*i/data_loader.__len__()))\n","                self.logger.info(msg)\n","            \n","            # DEBUGGING (remove break)\n","            break\n","\n","            ### Add some plotting functions here once code is running ###\n","\n","    def _train_step_baseline(self, data_loader, step):\n","    \n","        batch_losses = []\n","\n","        for i, (images, bias_labels, label_imgs) in enumerate(data_loader):\n","            \n","            # test_images = images.cpu().detach().numpy()\n","            # test_bias_labels = bias_labels.cpu().detach().numpy()\n","            # test_label_imgs = label_imgs.cpu().detach().numpy()\n","            # cv2_imshow(test_images) \n","            # cv2_imshow(test_bias_labels)\n","            # cv2_imshow(test_label_imgs)\n","\n","            images = self._get_variable(images)\n","            label_imgs = self._get_variable(label_imgs.type(torch.LongTensor))\n","\n","            self.optim.zero_grad()\n","            pred_map, softmax, bias_fork = self.net(images)\n","\n","            # Loss for self.net, semantic segmentation loss\n","            loss_pred = self.loss(pred_map, label_imgs)  # cross entropy of the final layer before softmax and the cityscapes black images\n","            # Create loss value for plotting\n","            loss_value = loss_pred.data.cpu().numpy()\n","            batch_losses.append(loss_value)\n","            \n","            # Optimiser step\n","            loss_pred.backward()\n","            self.optim.step()\n","\n","            # From deeplab script:\n","            # # compute the loss:\n","            # loss = loss_fn(outputs, label_imgs)\n","            # loss_value = loss.data.cpu().numpy() # this creates a numpy array on the cpu of the loss tenosr for plotting\n","            # batch_losses.append(loss_value)\n","\n","            # # optimization step:\n","            # optimizer.zero_grad() # (reset gradients)\n","            # loss.backward() # (compute gradients)\n","            # optimizer.step() # (perform optimization step)\n","\n","\n","            # TODO: print elapsed time for iteration\n","            if i % self.option.log_step == 0:\n","                msg = \"[TRAIN] cls loss : %.6f (epoch %d.%02d)\" \\\n","                    % (loss_pred,step,int(100*i/data_loader.__len__()))\n","                self.logger.info(msg)\n","            \n","            # add an output to the screen for debug to check greyscale is working\n","\n","            break        \n","        \n","        return batch_losses\n","\n","    def _validate(self, data_loader):\n","        self._mode_setting(is_train=False)\n","        self._initialization()\n","        if self.option.checkpoint is not None:\n","            self._load_model()\n","        else:\n","            print(\"No trained model for evaluation provided\")\n","            import sys\n","            sys.exit()\n","\n","        num_test = 10000\n","\n","        total_num_correct = 0.\n","        total_num_test = 0.\n","        total_loss = 0.\n","        for i, (images, bias_labels, label_imgs) in enumerate(data_loader):\n","            \n","            start_time = time.time()\n","            images = self._get_variable(images)\n","            bias_labels = self._get_variable(bias_labels)\n","            label_imgs = self._get_variable(label_imgs.type(torch.LongTensor))\n","\n","            self.optim.zero_grad()\n","            pred_map, softmax, bias_fork = self.net(images)\n","\n","\n","            loss = self.loss(pred_map, torch.squeeze(label_imgs)) #again not sure about squeeze check deeplab train script. \n","            \n","            batch_size = images.shape[0]\n","            total_num_correct += self._num_correct(pred_map, label_imgs, topk=1).data[0]\n","            total_loss += loss.data[0]*batch_size\n","            total_num_test += batch_size\n","               \n","        avg_loss = total_loss/total_num_test\n","        avg_acc = total_num_correct/total_num_test\n","        msg = \"EVALUATION LOSS  %.4f, ACCURACY : %.4f (%d/%d)\" % \\\n","                        (avg_loss,avg_acc,int(total_num_correct),total_num_test)\n","        self.logger.info(msg)\n","\n","\n","\n","    def _num_correct(self,outputs,labels,topk=1):\n","        _, preds = outputs.topk(k=topk, dim=1)\n","        preds = preds.t()\n","        correct = preds.eq(labels.view(1, -1).expand_as(preds))\n","        correct = correct.view(-1).sum()\n","        return correct\n","        \n","\n","\n","    def _accuracy(self, outputs, labels):\n","        batch_size = labels.size(0)\n","        _, preds = outputs.topk(k=1, dim=1)\n","        preds = preds.t()\n","        correct = preds.eq(labels.view(1, -1).expand_as(preds))\n","        correct = correct.view(-1).float().sum(0, keepdim=True)\n","        accuracy = correct.mul_(100.0 / batch_size)\n","        return accuracy\n","\n","    def _save_model(self, step): # this requires the directory to already be created (this is done in backend setting in main script)\n","        checkpoint_dir = self.path + '/checkpoint'\n","        if not os.path.exists(checkpoint_dir):\n","            os.makedirs(checkpoint_dir)\n","\n","        torch.save({\n","            'step': step,\n","            'optim_state_dict': self.optim.state_dict(),\n","            'net_state_dict': self.net.state_dict()\n","        }, os.path.join(checkpoint_dir, 'checkpoint_epoch_%04d.pth' % step))\n","        print('Checkpoint saved. Epoch : %d'%step)\n","\n","    def _load_model(self):\n","        ckpt = torch.load(self.option.checkpoint)\n","        self.net.load_state_dict(ckpt['net_state_dict']) # this is how we saved them in save model method above\n","        self.optim.load_state_dict(ckpt['optim_state_dict'])\n","\n","    # def _plotter_function(self, batch_losses, path):\n","    #       if self.option.is_train:\n","    #           title = 'train'\n","    #           filename = 'e\n","    #       else: \n","    #           title = 'val'     \n","    #     \n","    #     epoch_loss = np.mean(batch_losses)\n","    #     epoch_losses_train.append(epoch_loss)\n","    #     with open(\"%s/epoch_losses_%s.pkl\" % (path,title), \"wb\") as file:\n","    #         pickle.dump(epoch_losses_train, file)\n","    #     print (\"train loss: %g\" % epoch_loss)\n","    #     plt.figure(1)\n","    #     plt.plot(epoch_losses_train, \"k^\")\n","    #     plt.plot(epoch_losses_train, \"k\")\n","    #     plt.ylabel(\"loss\")\n","    #     plt.xlabel(\"epoch\")\n","    #     plt.title(\"%s loss per epoch\", % stage)\n","    #     plt.savefig(\"%s/epoch_losses_%s.png\" % (path, stage) )\n","    #     plt.close(1)\n","\n","    def train(self, train_loader, val_loader=None):\n","        self._initialization()\n","        if self.option.checkpoint is not None:\n","            self._load_model()\n","\n","        self._mode_setting(is_train=True)\n","        timer = Timer(self.logger, self.option.max_step)\n","        start_epoch = 0\n","        epoch_losses_train = []\n","        epoch_losses_val = []\n","        \n","        # if self.option.is_train:\n","        #     stage = 'train'\n","\n","        for step in range(start_epoch, self.option.max_step):\n","            if self.option.train_baseline:\n","                batch_losses = self._train_step_baseline(train_loader, step)\n","                print(batch_losses)\n","                #self._plotter_function(batch_losses, stage )\n","            else:\n","                self._train_step(train_loader,step) #LNTL proceedure\n","            self.scheduler.step()\n","            self.scheduler_r.step()\n","            self.scheduler_g.step()\n","            self.scheduler_b.step()\n","\n","            if step == 1 or step % self.option.save_step == 0 or step == (self.option.max_step-1):\n","                if val_loader is not None:\n","                    self._validate(val_loader) #removed step argument from here i dont think we need it\n","                self._save_model(step)\n","\n","\n","    def _get_variable(self, inputs):\n","        if self.option.cuda:\n","            return Variable(inputs.cuda()) #Is there a difference between Variable(inputs).cuda()??\n","        return Variable(inputs)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: import_ipynb in /usr/local/lib/python3.7/dist-packages (0.1.3)\n","importing Jupyter notebook from /content/drive/My Drive/BiasMitigation/LNTL/models/SegNet/segnet.ipynb\n","importing Jupyter notebook from /content/drive/My Drive/BiasMitigation/LNTL/models/BiasPredictor.ipynb\n","importing Jupyter notebook from /content/drive/My Drive/BiasMitigation/LNTL/utils/utils_LNTL.ipynb\n"],"name":"stdout"}]}]}