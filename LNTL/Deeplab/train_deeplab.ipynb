{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"train_deeplab.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m35YA_bwDHQ-","executionInfo":{"elapsed":96796,"status":"ok","timestamp":1623533774204,"user":{"displayName":"Jack Stelling","photoUrl":"","userId":"10755116166180930038"},"user_tz":-60},"outputId":"5eda17f8-42d1-4384-8df5-27ef58e8b556"},"source":["from google.colab import drive\n","#drive.mount('/content/drive', force_remount=True)\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dwOOFoanDQQC","executionInfo":{"elapsed":272,"status":"ok","timestamp":1623533871826,"user":{"displayName":"Jack Stelling","photoUrl":"","userId":"10755116166180930038"},"user_tz":-60},"outputId":"f141c03e-ea89-42e6-a50b-4921d475d539"},"source":["import os \n","os.chdir('/content/drive/MyDrive/BiasMitigation/LNTL')\n","print('Directory Changed to ...', os.getcwd())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Directory Changed\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"T9J68xfeFVU1"},"source":["\n","import sys\n","\n","# this next import comes from within our file structure (our datasets script)\n","! pip install import-ipynb \n","import import_ipynb\n","from data_loaders.data_loader_deeplab import DatasetTrain, DatasetVal # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\n","#import os\n","\n","# Mount a file stored on Google Drive:\n","#from google.colab import drive\n","#drive.mount('/content/drive/MyDrive/Deeplabv3Flat') \n","#Use code block at the top of page\n","\n","\n","# Dont need these commented out lines on Colab. File structutre is flat\n","#cwd = '/content/drive/MyDrive/Deeplabv3Flat'\n","\n","#os.chdir('/content/drive/MyDrive/Deeplabv3Flat/model')\n","#sys.path.append(\"/content/drive/MyDrive/Deeplabv3Flat/model\")\n","from deeplabv3 import DeepLabV3\n","\n","#os.chdir('/content/drive/MyDrive/Deeplabv3Flat/utils')\n","#sys.path.append(\"/content/drive/MyDrive/Deeplabv3Flat/utils\")\n","from utils import add_weight_decay\n","\n","#os.chdir(cwd)\n","\n","import torch\n","import torch.utils.data\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","import numpy as np\n","import pickle\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","import matplotlib.pyplot as plt\n","import cv2\n","\n","import time\n","from tqdm import tqdm\n","\n","\n","# NOTE! NOTE! change this to not overwrite all log data when you train the model:\n","model_id = \"4\" #these are now in option args\n","\n","num_epochs = 150\n","batch_size = 3   # changed from 3 to 24\n","learning_rate = 0.0001\n","\n","network = DeepLabV3(model_id, project_dir=\"/content/drive/MyDrive/Deeplabv3Flat\").cuda()\n","\n","# Amirs suggestion to resume training from last checkpoint #\n","\n","resume = True \n","\n","if resume == True:\n","     network.load_state_dict(torch.load(\"/content/drive/MyDrive/Deeplabv3Flat/training_logs/model_3/checkpoints/model_3_epoch_100.pth\"))\n","\n","#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","train_dataset = DatasetTrain(cityscapes_data_path=\"/content/drive/MyDrive/Datasets/Cityscapes\",\n","                             cityscapes_meta_path=\"/content/drive/MyDrive/Datasets/Cityscapes/meta\")\n","val_dataset = DatasetVal(cityscapes_data_path=\"/content/drive/MyDrive/Datasets/Cityscapes\",\n","                         cityscapes_meta_path=\"/content/drive/MyDrive/Datasets/Cityscapes/meta\")\n","\n","num_train_batches = int(len(train_dataset)/batch_size)\n","num_val_batches = int(len(val_dataset)/batch_size)\n","print (\"num_train_batches:\", num_train_batches)\n","print (\"num_val_batches:\", num_val_batches)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size, shuffle=True,\n","                                           num_workers=2) #changed from 1 to 0 ## as below \n","val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n","                                         batch_size=batch_size, shuffle=False,\n","                                         num_workers=2) # changed from 1 to 0 ## update changed to 10 for run 3\n","\n","params = add_weight_decay(network, l2_value=0.0001)\n","optimizer = torch.optim.Adam(params, lr=learning_rate)\n","\n","with open(\"/content/drive/MyDrive/Datasets/Cityscapes/meta/class_weights.pkl\", \"rb\") as file: # (needed for python3)\n","    class_weights = np.array(pickle.load(file))\n","class_weights = torch.from_numpy(class_weights)\n","class_weights = Variable(class_weights.type(torch.FloatTensor)).cuda()\n","\n","# loss function\n","loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n","\n","epoch_losses_train = []\n","epoch_losses_val = []\n","for epoch in range(num_epochs):\n","    print (\"###########################\")\n","    print (\"######## NEW EPOCH ########\")\n","    print (\"###########################\")\n","    print (\"epoch: %d/%d\" % (epoch+1, num_epochs))\n","\n","    ############################################################################\n","    # train:\n","    ############################################################################\n","    network.train() # (set in training mode, this affects BatchNorm and dropout)\n","    batch_losses = []\n","    \n","    # found this from aladdin person youtube 'add progress bar to pytorch'\n","    loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n","    \n","    for step, (imgs, label_imgs, bias_labels) in loop:\n","        current_time = time.time()\n","\n","        imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n","        label_imgs = Variable(label_imgs.type(torch.LongTensor)).cuda() # (shape: (batch_size, img_h, img_w))\n","\n","        outputs, _ , _ = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))   (outputs, softmax, bias_fork)\n","\n","        # compute the loss:\n","        loss = loss_fn(outputs, label_imgs)\n","        loss_value = loss.data.cpu().numpy()\n","        batch_losses.append(loss_value)\n","\n","        # optimization step:\n","        optimizer.zero_grad() # (reset gradients)\n","        loss.backward() # (compute gradients)\n","        optimizer.step() # (perform optimization step)\n","\n","        # Update Progress Bar\n","        loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n","        loop.set_postfix(loss = loss.item())\n","        print (time.time() - current_time)\n","\n","    epoch_loss = np.mean(batch_losses)\n","    epoch_losses_train.append(epoch_loss)\n","    with open(\"%s/epoch_losses_train.pkl\" % network.model_dir, \"wb\") as file:\n","        pickle.dump(epoch_losses_train, file)\n","    print (\"train loss: %g\" % epoch_loss)\n","    plt.figure(1)\n","    plt.plot(epoch_losses_train, \"k^\")\n","    plt.plot(epoch_losses_train, \"k\")\n","    plt.ylabel(\"loss\")\n","    plt.xlabel(\"epoch\")\n","    plt.title(\"train loss per epoch\")\n","    plt.savefig(\"%s/epoch_losses_train.png\" % network.model_dir)\n","    plt.close(1)\n","\n","    print (\"####\")\n","\n","    \n","    \n","    ############################################################################\n","    # val:\n","    ############################################################################\n","    network.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n","    batch_losses = []\n","    for step, (imgs, label_imgs, img_ids, bias_labels) in enumerate(val_loader):\n","        with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n","            imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n","            label_imgs = Variable(label_imgs.type(torch.LongTensor)).cuda() # (shape: (batch_size, img_h, img_w))\n","\n","            outputs, _ , _ = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))   (outputs, softmax, bias_fork)\n","\n","            # compute the loss:\n","            loss = loss_fn(outputs, label_imgs)\n","            loss_value = loss.data.cpu().numpy()\n","            batch_losses.append(loss_value)\n","\n","    epoch_loss = np.mean(batch_losses)\n","    epoch_losses_val.append(epoch_loss)\n","    with open(\"%s/epoch_losses_val.pkl\" % network.model_dir, \"wb\") as file:\n","        pickle.dump(epoch_losses_val, file)\n","    print (\"val loss: %g\" % epoch_loss)\n","    plt.figure(1)\n","    plt.plot(epoch_losses_val, \"k^\")\n","    plt.plot(epoch_losses_val, \"k\")\n","    plt.ylabel(\"loss\")\n","    plt.xlabel(\"epoch\")\n","    plt.title(\"val loss per epoch\")\n","    plt.savefig(\"%s/epoch_losses_val.png\" % network.model_dir)\n","    plt.close(1)\n","\n","    # save the model weights to disk:\n","    checkpoint_path = network.checkpoints_dir + \"/model_\" + model_id +\"_epoch_\" + str(epoch+1) + \".pth\"\n","    torch.save(network.state_dict(), checkpoint_path)\n","    \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TbSslEhbFVU-","executionInfo":{"elapsed":280,"status":"ok","timestamp":1623164890753,"user":{"displayName":"Jack Stelling","photoUrl":"","userId":"10755116166180930038"},"user_tz":-60},"outputId":"33064e98-b330-4329-db2b-b27b87564713"},"source":["os.listdir('/content/drive/MyDrive/Deeplabv3Flat/pretrained_models/resnet')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['resnet18-5c106cde.pth', 'resnet34-333f7ec4.pth', 'resnet50-19c8e357.pth']"]},"metadata":{"tags":[]},"execution_count":7}]}]}