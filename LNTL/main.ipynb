{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOIixNPJHiT5z1f6zVjgrg4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rx5EjcP5wo1H","executionInfo":{"status":"ok","timestamp":1628589614267,"user_tz":-60,"elapsed":41497,"user":{"displayName":"Jack Stelling","photoUrl":"","userId":"10755116166180930038"}},"outputId":"545aeea6-e760-401b-dc3f-79811f5d77bb"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hMMunGwI1gAD","executionInfo":{"status":"ok","timestamp":1628589647679,"user_tz":-60,"elapsed":872,"user":{"displayName":"Jack Stelling","photoUrl":"","userId":"10755116166180930038"}},"outputId":"cb7246d0-f16a-4d65-8688-291f0171fc70"},"source":["# If using a remote server it is neccessary to navigate to your workspace\n","import os \n","\n","os.chdir('/content/drive/MyDrive/BiasMitigation/LNTL')\n","print('Directory changed to...',  os.getcwd())"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Directory changed to... /content/drive/MyDrive/BiasMitigation/LNTL\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c9A8YypuAfBY","colab":{"base_uri":"https://localhost:8080/","height":663},"executionInfo":{"status":"error","timestamp":1628589793284,"user_tz":-60,"elapsed":135766,"user":{"displayName":"Jack Stelling","photoUrl":"","userId":"10755116166180930038"}},"outputId":"6b572b47-a202-4b50-ffdd-345f79ddc69d"},"source":["# -*- coding: utf-8 -*-\n","\n","# file imports\n","! pip install import-ipynb \n","import import_ipynb\n","import random\n","from option import get_option\n","from trainer_merger import *\n","from utils.utils_LNTL import save_option\n","from data_loaders.data_loader_cityscapes import *\n","from data_loaders.data_loader_SYNTHIA import *\n","\n","# torch imports\n","import torch\n","from torch.backends import cudnn\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torch.utils.data as data\n","\n","# argparse jupyter workaround\n","from argparse import Namespace\n","# manually add the options becuase using juypter not command line:\n","# Must make it a Namespace class to be used as the argparser would be\n","root = '/content/drive/MyDrive/BiasMitigation/'\n","\n","args = Namespace(exp_name = 'Exp1_Stage1', # Enter in format Exp1_Stage2\n","                 batch_size = 4,\n","                 momentum = 0.9,\n","                 optimiser = 'ADAM',  # accepts either ADAM or SGD\n","                 lr = 0.0001,  # try lr = 0.0001 for ADAM and lr = 0.001 SGD \n","                 lr_decay_rate = 0.1,\n","                 lr_decay_period = 40,\n","                 weight_decay = 0.0001, #arbitrary value off net doubt we have time to tune. would like some paper to back up this ebing a good value\n","                 max_step = 100,  \n","                 seed = 2,\n","                 ##############\n","                 log_step = 100 ,\n","                 save_step = 1, # save every epoch\n","                 dataset = 'SYNTHIA', # Accepts 'Cityscapes' or 'SYNTHIA' only\n","                 data_dir = root + 'Datasets/Cityscapes/',\n","                 meta_dir = root + 'Datasets/Cityscapes/meta',\n","                 save_dir = root + 'LNTL/training_logs',\n","                 data_split = 'train',\n","                 use_pretrain = False, \n","                 train_baseline = True,\n","                 train_greyscale = False,\n","                 train_bias_head_only = False,\n","                 #############\n","                 checkpoint = None, # root + 'LNTL/training_logs/Deeplab/Cityscapes/LNTL/Exp5_Stage1/checkpoints/checkpoint_bias_head_epoch_0039.pth', \n","                 load_bias_head_only = False,\n","                 load_seg_head_only = False,\n","                 #############\n","                 val_only_greyscale = False,\n","                 val_only_jitter = False,\n","                 val_only_invert = False,\n","                 #############\n","                 random_seed = None,\n","                 num_workers = 2,\n","                 cudnn_benchmark = True,\n","                 #############\n","                 cuda = True ,\n","                 debug = False, \n","                 is_train = True,  \n","                 #############\n","                 network_type = 'Deeplab',  # Enter only Deeplab or SegNet\n","                 notes = 'First attempt at running SYNTHIA data though model.'\n","                 )\n","\n","\n","'''\n","parser.add_argument('-e', '--exp_name',   required=True,              help='experiment name')\n","\n","parser.add_argument('--n_class',          default=10,     type=int,   help='number of classes')\n","parser.add_argument('--input_size',       default=28,     type=int,   help='input size')\n","parser.add_argument('--batch_size',       default=128,    type=int,   help='mini-batch size')\n","parser.add_argument('--momentum',         default=0.9,    type=float, help='sgd momentum')\n","parser.add_argument('--lr',               default=0.01,   type=float, help='initial learning rate')\n","parser.add_argument('--lr_decay_rate',    default=0.1,    type=float, help='lr decay rate')\n","parser.add_argument('--lr_decay_period',  default=40,     type=int,   help='lr decay period')\n","parser.add_argument('--weight_decay',     default=0.0005, type=float, help='sgd optimizer weight decay')\n","parser.add_argument('--max_step',         default=100,    type=int,   help='maximum step for training')\n","parser.add_argument('--depth',            default=20,     type=int,   help='depth of network')\n","parser.add_argument('--color_var',        default=0.03,   type=float, help='variance for color distribution')\n","parser.add_argument('--seed',             default=2,      type=int,   help='seed index')\n","\n","\n","parser.add_argument('--checkpoint',       default=None,               help='checkpoint to resume')\n","parser.add_argument('--log_step',         default=50,     type=int,   help='step for logging in iteration')\n","parser.add_argument('--save_step',        default=10,     type=int,   help='step for saving in epoch')\n","parser.add_argument('--data_dir',         default='./',               help='data directory')\n","parser.add_argument('--save_dir',         default='./',               help='save directory for checkpoint')\n","parser.add_argument('--data_split',       default='train',            help='data split to use')\n","parser.add_argument('--use_pretrain',     action='store_true',        help='whether it use pre-trained parameters if exists')\n","parser.add_argument('--train_baseline',   action='store_true',        help='whether it train baseline or unlearning')\n","parser.add_argument('--train_greyscale',  action='store_true',        help='whether to convert the images to greyscale to get a measure of no colour information')\n","\n","parser.add_argument('--random_seed',                      type=int,   help='random seed')\n","parser.add_argument('--num_workers',      default=4,      type=int,   help='number of workers in data loader')\n","parser.add_argument('--cudnn_benchmark',  default=True,   type=bool,  help='cuDNN benchmark')\n","\n","\n","parser.add_argument('--cuda',             action='store_true',        help='enables cuda')\n","parser.add_argument('-d', '--debug',      action='store_true',        help='debug mode')\n","parser.add_argument('--is_train',         action='store_true',        help='whether it is training')\n","'''\n","\n","def log_file_setup(option):\n","    # setting up training log files:\n","    if option.train_baseline:\n","        if option.train_greyscale:\n","            kind = 'baseline_greyscale'\n","            path = option.save_dir + '/' + option.network_type + '/' + option.dataset + '/baseline' + '/greyscale' + '/' + option.exp_name \n","            if not os.path.exists(path):\n","                os.makedirs(path)\n","        else:\n","            kind = 'baseline_colour'\n","            # path = os.path.join(option.save_dir, '/' , option.network_type, '/', option.dataset, '/baseline', '/colour', '/', option.exp_name )\n","            path = option.save_dir + '/' + option.network_type + '/' + option.dataset + '/baseline' + '/colour' + '/' + option.exp_name\n","            if not os.path.exists(path):\n","                os.makedirs(path)\n","    else:\n","        kind = 'LNTL'\n","        #path = os.path.join(option.save_dir, '/' , option.network_type, '/', option.dataset, '/LNTL', '/', option.exp_name )\n","        path = option.save_dir + '/' + option.network_type + '/' + option.dataset + '/LNTL' + '/' + option.exp_name\n","        if not os.path.exists(path):\n","            os.makedirs(path)\n","    \n","    return path\n","\n","\n","def backend_setting(option):\n","\n","    if option.random_seed is None:\n","        option.random_seed = random.randint(1,10000)\n","    torch.manual_seed(option.random_seed)\n","\n","    if torch.cuda.is_available() and not option.cuda:\n","        print('WARNING: GPU is available, but we are not using it')\n","\n","    if not torch.cuda.is_available() and option.cuda:\n","        option.cuda = False\n","        print('Warning: Youve asked for CUDA but no GPU is available...setting CUDA to False')\n","\n","    if option.cuda:\n","        os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n","        #os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(i) for i in option.gpu_ids])\n","        torch.cuda.manual_seed_all(option.random_seed)\n","        cudnn.benchmark = option.cudnn_benchmark\n","    \n","    if option.train_baseline:\n","        option.is_train = True\n","\n","\n","def main():\n","    #option = get_option()\n","    option = args\n","    #debug:\n","    #print(option)\n","    \n","    path = log_file_setup(option)\n","    backend_setting(option)\n","    trainer = Trainer(option, path)\n","\n","    if option.dataset == 'Cityscapes':\n","        train_dataset = DatasetTrain(option)\n","        val_dataset = DatasetVal(option)\n","    elif option.dataset == 'SYNTHIA':\n","        train_dataset = DatasetTrain_SYNTHIA(option)\n","        val_dataset = DatasetVal_SYNTHIA(option)\n","\n","\n","    train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n","                                                batch_size=option.batch_size,\n","                                                shuffle=True,\n","                                                num_workers=option.num_workers)\n","\n","    val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n","                                                batch_size=option.batch_size,\n","                                                shuffle=False,\n","                                                num_workers=option.num_workers)\n","\n","    if option.is_train:   \n","        save_option(option, path)\n","        trainer.train(train_loader = train_loader, val_loader = val_loader)\n","    else:\n","        trainer._validate(trainval_loader) # I dont think we need this\n","\n","        pass\n","\n","if __name__ == '__main__': main()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting import-ipynb\n","  Downloading import-ipynb-0.1.3.tar.gz (4.0 kB)\n","Building wheels for collected packages: import-ipynb\n","  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-py3-none-any.whl size=2975 sha256=ae952e064e6535970076cad29dd054c6b95d25cb112f6e57ba08aa15fcb29ce6\n","  Stored in directory: /root/.cache/pip/wheels/b1/5e/dc/79780689896a056199b0b9f24471e3ee184fbd816df355d5f0\n","Successfully built import-ipynb\n","Installing collected packages: import-ipynb\n","Successfully installed import-ipynb-0.1.3\n","importing Jupyter notebook from option.ipynb\n","importing Jupyter notebook from trainer_merger.ipynb\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Changed the local path to.... /content/drive/MyDrive/BiasMitigation/LNTL\n","Requirement already satisfied: import_ipynb in /usr/local/lib/python3.7/dist-packages (0.1.3)\n","importing Jupyter notebook from /content/drive/My Drive/BiasMitigation/LNTL/models/Deeplab/deeplabv3.ipynb\n","importing Jupyter notebook from /content/drive/My Drive/BiasMitigation/LNTL/models/Deeplab/resnet.ipynb\n","importing Jupyter notebook from /content/drive/My Drive/BiasMitigation/LNTL/models/Deeplab/aspp.ipynb\n","importing Jupyter notebook from /content/drive/My Drive/BiasMitigation/LNTL/models/SegNet/segnet.ipynb\n","importing Jupyter notebook from /content/drive/My Drive/BiasMitigation/LNTL/models/biashead.ipynb\n","importing Jupyter notebook from /content/drive/My Drive/BiasMitigation/LNTL/utils/utils_LNTL.ipynb\n","importing Jupyter notebook from /content/drive/My Drive/BiasMitigation/LNTL/utils/utils_Deeplab.ipynb\n","importing Jupyter notebook from /content/drive/My Drive/BiasMitigation/LNTL/data_loaders/data_loader_cityscapes.ipynb\n","importing Jupyter notebook from /content/drive/My Drive/BiasMitigation/LNTL/data_loaders/data_loader_SYNTHIA.ipynb\n","pretrained resnet, 18\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-ce8daa5914e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-ce8daa5914e2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetVal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'SYNTHIA'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetTrain_SYNTHIA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetVal_SYNTHIA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/BiasMitigation/LNTL/data_loaders/data_loader_SYNTHIA.ipynb\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, option)\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"]}]}]}