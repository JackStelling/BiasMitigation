{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"trainer_merger.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPdAXUBwNEjtsA2GD88B+/Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4q5B9A7uabXu","executionInfo":{"status":"ok","timestamp":1627044671853,"user_tz":-60,"elapsed":19434,"user":{"displayName":"Jack Stelling","photoUrl":"","userId":"10755116166180930038"}},"outputId":"9ef7146d-c126-4ea5-9de6-4c8c0bdd4512"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lrjM8xv4a36f","executionInfo":{"status":"ok","timestamp":1627044757720,"user_tz":-60,"elapsed":216,"user":{"displayName":"Jack Stelling","photoUrl":"","userId":"10755116166180930038"}},"outputId":"4d14a532-4437-467f-8cf5-fc05bf951d58"},"source":["import os\n","os.chdir('/content/drive/MyDrive/BiasMitigation/LNTL')\n","print('Changed the local path to....', os.getcwd())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Changed the local path to.... /content/drive/MyDrive/BiasMitigation/LNTL\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7Xoqp4k5_cmt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627044891272,"user_tz":-60,"elapsed":7966,"user":{"displayName":"Jack Stelling","photoUrl":"","userId":"10755116166180930038"}},"outputId":"df18794f-86e7-4f27-ae79-cb64e2bb3768"},"source":["# Local scripts\n","! pip install import_ipynb\n","\n","import import_ipynb\n","\n","from models.Deeplab import deeplabv3\n","from models.SegNet import segnet\n","from models import biashead\n","from utils.utils_LNTL import logger_setting, Timer\n","from utils.utils_Deeplab import add_weight_decay\n","# Python\n","import time\n","import os\n","import math\n","import numpy as np\n","import numpy as np\n","import pickle\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","import matplotlib.pyplot as plt\n","# Torch\n","import torch\n","from torch import nn\n","from torch import optim\n","from torch.autograd import Variable\n","# Colab \n","from google.colab.patches import cv2_imshow\n","\n","\n","class GradReverse(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, x):\n","        return x.view_as(x)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        return grad_output.neg() * 0.1\n","\n","def grad_reverse(x):\n","    return GradReverse.apply(x)\n","\n","\n","\n","class Trainer(object):\n","    def __init__(self, option, path):\n","        self.option = option\n","        self.path = path\n","\n","        self._build_model()\n","        self._set_optimizer()\n","        self.logger = logger_setting(option.exp_name, option.save_dir, path, option.debug)\n","\n","    def _build_model(self):\n","        self.n_color_cls = 8    # This is how many colour bins we have used 256/32\n","\n","        # The bias input channels depend on fork placement \n","        if self.option.network_type == 'Deeplab': # CAN WE JUST USE option.network_type HERE BECUASE OF LINE 31\n","            bias_input_channels = 1280  # The number of feature maps after the aspp concat step.\n","        elif self.option.network_type == 'Segnet':\n","            bias_input_channels = 64\n","\n","        #self.net = deeplabv3.DeepLabV3(model_id=self.option.exp_name, project_dir = self.option.save_dir) changed from this becuase i changed the arguements in DeepLabV3\n","        self.net = deeplabv3.DeepLabV3() # Doesnt need number of classes as its hard coded at the top of the deeplab class\n","        # The path might have to be changed since we have a directory models with the models inside. Rather than a script with the models inside as this is being called.\n","        # Also the deeplab class calls Aspp and Resnet so check paths in those scripts. \n","        self.pred_net_r = biashead.BiasPredictor(input_ch=bias_input_channels, num_classes=self.n_color_cls)\n","        self.pred_net_g = biashead.BiasPredictor(input_ch=bias_input_channels, num_classes=self.n_color_cls)\n","        self.pred_net_b = biashead.BiasPredictor(input_ch=bias_input_channels, num_classes=self.n_color_cls)\n","\n","\n","        with open( self.option.meta_dir + \"/class_weights.pkl\", \"rb\") as file: # (needed for python3)\n","            class_weights = np.array(pickle.load(file))\n","        class_weights = torch.from_numpy(class_weights)\n","        class_weights = Variable(class_weights.type(torch.FloatTensor)).cuda()\n","\n","\n","        self.loss = nn.CrossEntropyLoss(weight = class_weights)  # must add weights to this one to perform cross entropy relatively (a traffic light is as important as a building)\n","        self.color_loss = nn.CrossEntropyLoss()\n","\n","\n","        if self.option.cuda:\n","            self.net.cuda()\n","            self.pred_net_r.cuda()\n","            self.pred_net_g.cuda()\n","            self.pred_net_b.cuda()\n","            self.loss.cuda()\n","            self.color_loss.cuda()\n","\n","    def _set_optimizer(self): # worth adding in ADAM here?\n","        if self.option.optimiser == 'ADAM':\n","            params_net = add_weight_decay(self.net, l2_value = self.option.weight_decay) \n","            params_r = add_weight_decay(self.pred_net_r, l2_value = self.option.weight_decay) \n","            params_g = add_weight_decay(self.pred_net_g, l2_value = self.option.weight_decay) \n","            params_b = add_weight_decay(self.pred_net_b, l2_value = self.option.weight_decay) \n","            #####\n","            self.optim = optim.Adam(params_net, lr=self.option.lr)\n","            self.optim_r = optim.Adam(params_r, lr=self.option.lr)\n","            self.optim_g = optim.Adam(params_g, lr=self.option.lr)\n","            self.optim_b = optim.Adam(params_b, lr=self.option.lr)\n","        elif self.option.optimiser == 'SGD':\n","            self.optim = optim.SGD(filter(lambda p: p.requires_grad, self.net.parameters()), lr=self.option.lr, momentum=self.option.momentum, weight_decay=self.option.weight_decay)\n","            self.optim_r = optim.SGD(self.pred_net_r.parameters(), lr=self.option.lr, momentum=self.option.momentum, weight_decay=self.option.weight_decay)\n","            self.optim_g = optim.SGD(self.pred_net_g.parameters(), lr=self.option.lr, momentum=self.option.momentum, weight_decay=self.option.weight_decay)\n","            self.optim_b = optim.SGD(self.pred_net_b.parameters(), lr=self.option.lr, momentum=self.option.momentum, weight_decay=self.option.weight_decay)\n","\n","        #TODO: last_epoch should be the last step of loaded model\n","        lr_lambda = lambda step: self.option.lr_decay_rate ** (step // self.option.lr_decay_period)\n","        self.scheduler = optim.lr_scheduler.LambdaLR(self.optim, lr_lambda=lr_lambda, last_epoch=-1)\n","        self.scheduler_r = optim.lr_scheduler.LambdaLR(self.optim_r, lr_lambda=lr_lambda, last_epoch=-1)\n","        self.scheduler_g = optim.lr_scheduler.LambdaLR(self.optim_g, lr_lambda=lr_lambda, last_epoch=-1)\n","        self.scheduler_b = optim.lr_scheduler.LambdaLR(self.optim_b, lr_lambda=lr_lambda, last_epoch=-1)\n","\n","    @staticmethod\n","    def _weights_init(m):\n","        classname = m.__class__.__name__\n","        if classname.find('Conv') != -1:\n","            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            m.weight.data.normal_(0, math.sqrt(2. / n))\n","        elif classname.find('BatchNorm') != -1:\n","            m.weight.data.fill_(1.0)\n","            m.bias.data.zero_()\n","\n","    def _initialization(self):\n","        self.net.apply(self._weights_init)\n","\n","\n","        if self.option.is_train and self.option.use_pretrain:\n","            if self.option.checkpoint is not None:\n","                self._load_model()\n","            else:\n","                print(\"Pre-trained model not provided\")\n","\n","\n","\n","    def _mode_setting(self, is_train=True):\n","        if is_train:\n","            self.net.train()\n","            self.pred_net_r.train()\n","            self.pred_net_g.train()\n","            self.pred_net_b.train()\n","        else:\n","            self.net.eval()\n","            self.pred_net_r.eval()\n","            self.pred_net_g.eval()\n","            self.pred_net_b.eval()\n","\n","\n","\n","    def _train_step(self, data_loader, step):\n","        _lambda = 0.01    # should we make this an option?\n","        self._mode_setting(is_train=True) # Put in train mode\n","        batch_losses_bias = []   # Reset bias batch losses\n","        batch_losses_seg = []   # Reset segmentation batch losses\n","\n","        for i, (images, bias_labels, label_imgs) in enumerate(data_loader):\n","            \n","            images = self._get_variable(images)\n","            bias_labels = self._get_variable(bias_labels)\n","            label_imgs = self._get_variable(label_imgs.type(torch.LongTensor))  # converts to pytorch variable, uses CUDA if GPU available. \n","\n","            # reset gradients \n","            self.optim.zero_grad()\n","            self.optim_r.zero_grad()\n","            self.optim_g.zero_grad()\n","            self.optim_b.zero_grad()\n","            pred_map, softmax, bias_fork = self.net(images)\n","\n","\n","            # predict colors from feat_label. Their prediction should be uniform.\n","            _,pseudo_pred_r = self.pred_net_r(bias_fork)  # outputs x and p(x),  here we get the softmax output\n","            _,pseudo_pred_g = self.pred_net_g(bias_fork)\n","            _,pseudo_pred_b = self.pred_net_b(bias_fork)\n","\n","\n","            # loss for self.net - semantic segmentation\n","            loss_pred = self.loss(pred_map, label_imgs) #the had a torch.squeeze here previously\n","\n","            loss_pseudo_pred_r = torch.mean(torch.sum(pseudo_pred_r*torch.log(pseudo_pred_r),1))  # manual cross entropy p(x)log(p(x)) missing the negative sign. \n","            loss_pseudo_pred_g = torch.mean(torch.sum(pseudo_pred_g*torch.log(pseudo_pred_g),1))\n","            loss_pseudo_pred_b = torch.mean(torch.sum(pseudo_pred_b*torch.log(pseudo_pred_b),1))\n","            \n","            \n","            loss_pred_ps_color = (loss_pseudo_pred_r + loss_pseudo_pred_g + loss_pseudo_pred_b) / 3.\n","            loss = loss_pred + loss_pred_ps_color*_lambda\n","            \n","            # DEBUGGING: #####\n","            print('**************************')\n","            #print('pseudo_pred_r.... \\n', pseudo_pred_r)\n","            print('pseudo_pred_r.shape =  ', pseudo_pred_r.shape)\n","            print('**************************')\n","            #print('loss_pseudo_pred_r  \\n', loss_pseudo_pred_r)\n","            print('loss_pseudo_pred_r.shape = ', loss_pseudo_pred_r.shape)\n","            print('**************************')\n","            #print('loss_pred_ps_color \\n', loss_pred_ps_color)\n","            print('loss_pred_ps_color.shape  = ', loss_pred_ps_color.shape)\n","            print('**************************')\n","            print('loss', loss)\n","            ###################\n","            # Create loss value for plotting\n","            loss_value = loss_pred.data.cpu().numpy()  # this creates a numpy array on the cpu of the loss tensor for plotting\n","            batch_losses_seg.append(loss_value) \n","\n","            loss.backward()\n","            self.optim.step()\n","\n","            # Reset gradients for the next stage of training schema\n","            self.optim.zero_grad()\n","            self.optim_r.zero_grad()\n","            self.optim_g.zero_grad()\n","            self.optim_b.zero_grad()\n","\n","            pred_map, softmax, bias_fork = self.net(images)\n","            feat_color = grad_reverse(bias_fork)\n","            \n","            pred_r,_ = self.pred_net_r(feat_color)  # outputs x, p(x),  this time we get the layer before the softmax\n","            pred_g,_ = self.pred_net_g(feat_color)\n","            pred_b,_ = self.pred_net_b(feat_color)\n","\n","            # loss for rgb predictors\n","            loss_pred_r = self.color_loss(pred_r, bias_labels[:,0]) # colour_loss() is the cross entropy instance, bias_labels are the ground truths created in dataloaders, [:,0] strips out R component \n","            loss_pred_g = self.color_loss(pred_g, bias_labels[:,1])\n","            loss_pred_b = self.color_loss(pred_b, bias_labels[:,2])\n","\n","            loss_pred_color = loss_pred_r + loss_pred_g + loss_pred_b\n","\n","            # Create loss value for plotting\n","            loss_value = loss_pred_color.data.cpu().numpy()  # this creates a numpy array on the cpu of the loss tensor for plotting\n","            batch_losses_bias.append(loss_value) \n","\n","            loss_pred_color.backward()\n","            self.optim.step()\n","            self.optim_r.step()\n","            self.optim_g.step()\n","            self.optim_b.step()\n","\n","            if i % self.option.log_step == 0:\n","                msg = \"[TRAIN] cls loss : %.6f, rgb : %.6f, MI : %.6f  (epoch %d.%02d)\" \\\n","                       % (loss_pred,loss_pred_color/3.,loss_pred_ps_color,step,int(100*i/data_loader.__len__()))\n","                self.logger.info(msg)\n","        \n","        return batch_losses_seg, batch_losses_bias \n","    \n","    def _validate_step(self, data_loader, step):\n","        self._mode_setting(is_train=False)\n","        batch_losses_seg = []\n","        batch_losses_bias = []\n","\n","        for i, (images, bias_labels, label_imgs, img_id) in enumerate(data_loader):\n","            with torch.no_grad():\n","                images = self._get_variable(images)\n","                bias_labels = self._get_variable(bias_labels)\n","                label_imgs = self._get_variable(label_imgs.type(torch.LongTensor))\n","\n","                # Segmentation Head\n","                # self.optim.zero_grad() dont need becuase its with torch.no_grad()\n","                pred_map, softmax , bias_fork = self.net(images) \n","\n","                # Loss for self.net, semantic segmentation loss\n","                loss_pred = self.loss(pred_map, label_imgs)  # cross entropy of the final layer before softmax and the cityscapes black images\n","                #debugging\n","                print('computed loss_pred in _validate_step...',  loss_pred)\n","                # Create loss value for plotting\n","                loss_value = loss_pred.data.cpu().numpy()  # this creates a numpy array on the cpu of the loss tensor for plotting\n","                batch_losses_seg.append(loss_value) \n","            \n","                # Bias Head\n","                pred_r,_ = self.pred_net_r(bias_fork)  # outputs x, p(x),  this time we get the layer before the softmax\n","                pred_g,_ = self.pred_net_g(bias_fork)\n","                pred_b,_ = self.pred_net_b(bias_fork)\n","                print('pred_r.shape...', pred_r.shape)\n","                print('pred_g.shape...', pred_g.shape)\n","                print('pred_b.shape...', pred_b.shape)\n","                #~~~~~\n","                print('pred_map.shape ......', pred_map.shape)\n","                print('bias_fork.shape ......', bias_fork.shape)\n","                print('softmax.shape ......', softmax.shape)\n","                print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n","                print('step....', i)\n","                print('images.shape ......', pred_map.shape)\n","                print('bias_labels.shape ......', bias_labels.shape)\n","                print('label_imgs.shape ......', label_imgs.shape)\n","                print('bias_labels[:,0].shape....', bias_labels[:,0].shape )\n","                #~~~~~\n","                loss_pred_r = self.color_loss(pred_r, bias_labels[:,0]) # colour_loss() is the cross entropy instance, bias_labels are the ground truths created in dataloaders, [:,0] strips out R component \n","                print('computed loss_pred_r in _validate_step...',  loss_pred_r)\n","                loss_pred_g = self.color_loss(pred_g, bias_labels[:,1])\n","                print('computed loss_pred_g in _validate_step...',  loss_pred_g)\n","                loss_pred_b = self.color_loss(pred_b, bias_labels[:,2])\n","                print('computed loss_pred_b in _validate_step...',  loss_pred_b)\n","\n","                loss_pred_color = loss_pred_r + loss_pred_g + loss_pred_b\n","\n","                # Create loss value for plotting\n","                loss_value = loss_pred_color.data.cpu().numpy()  # this creates a numpy array on the cpu of the loss tensor for plotting\n","                batch_losses_bias.append(loss_value) \n","\n","        \n","            # TODO: print elapsed time for iteration\n","            if i % self.option.log_step == 0:\n","                msg = \"[VAL] cls loss : %.6f (epoch %d.%02d)\" \\\n","                    % (loss_pred,step,int(100*i/data_loader.__len__()))\n","                self.logger.info(msg)\n","               \n","        return batch_losses_seg, batch_losses_bias\n","\n","    def _train_step_baseline(self, data_loader, step):\n","        \n","        self._mode_setting(is_train=True) # Put in train mode\n","        batch_losses = []   # Reset batch losses\n","\n","        for i, (images, bias_labels, label_imgs) in enumerate(data_loader):\n","            \n","            # test_images = images.cpu().detach().numpy()\n","            # test_bias_labels = bias_labels.cpu().detach().numpy()\n","            # test_label_imgs = label_imgs.cpu().detach().numpy()\n","            # cv2_imshow(test_images) \n","            # cv2_imshow(test_bias_labels)\n","            # cv2_imshow(test_label_imgs)\n","\n","            images = self._get_variable(images)\n","            label_imgs = self._get_variable(label_imgs.type(torch.LongTensor))\n","\n","            self.optim.zero_grad()\n","            pred_map, softmax, bias_fork = self.net(images) \n","\n","            # Loss for self.net, semantic segmentation loss\n","            loss_pred = self.loss(pred_map, label_imgs)  # cross entropy of the final layer before softmax and the cityscapes black images\n","            # Create loss value for plotting\n","            loss_value = loss_pred.data.cpu().numpy()  # this creates a numpy array on the cpu of the loss tensor for plotting\n","            batch_losses.append(loss_value) \n","            \n","            # Optimiser step\n","            loss_pred.backward()\n","            self.optim.step() # gradient descent\n","\n","\n","            # TODO: print elapsed time for iteration\n","            if i % self.option.log_step == 0:\n","                msg = \"[TRAIN] cls loss : %.6f (epoch %d.%02d)\" \\\n","                    % (loss_pred,step,int(100*i/data_loader.__len__()))\n","                self.logger.info(msg)\n","               \n","        return batch_losses\n","\n","    def _validate_step_baseline(self, data_loader, step):\n","        self._mode_setting(is_train=False)\n","        batch_losses = []\n","        \n","        for i, (images, bias_labels, label_imgs, img_id) in enumerate(data_loader):\n","            with torch.no_grad():\n","                images = self._get_variable(images)\n","                label_imgs = self._get_variable(label_imgs.type(torch.LongTensor))\n","\n","                self.optim.zero_grad()\n","                pred_map, _ , _ = self.net(images) \n","\n","                # Loss for self.net, semantic segmentation loss\n","                loss_pred = self.loss(pred_map, label_imgs)  # cross entropy of the final layer before softmax and the cityscapes black images\n","                # Create loss value for plotting\n","                loss_value = loss_pred.data.cpu().numpy()  # this creates a numpy array on the cpu of the loss tensor for plotting\n","                batch_losses.append(loss_value) \n","            \n","            # TODO: print elapsed time for iteration\n","            if i % self.option.log_step == 0:\n","                msg = \"[VAL] cls loss : %.6f (epoch %d.%02d)\" \\\n","                    % (loss_pred,step,int(100*i/data_loader.__len__()))\n","                self.logger.info(msg)\n","               \n","        return batch_losses\n","    \n","    # def _validate(self, data_loader):\n","    #     self._mode_setting(is_train=False)\n","    #     self._initialization()\n","    #     if self.option.checkpoint is not None:\n","    #         self._load_model()\n","    #     else:\n","    #         print(\"No trained model for evaluation provided\")\n","    #         import sys\n","    #         sys.exit()\n","\n","    #     num_test = 10000\n","\n","    #     total_num_correct = 0.\n","    #     total_num_test = 0.\n","    #     total_loss = 0.\n","    #     for i, (images, bias_labels, label_imgs, img_id) in enumerate(data_loader):\n","            \n","    #         start_time = time.time()\n","    #         images = self._get_variable(images)\n","    #         bias_labels = self._get_variable(bias_labels)\n","    #         label_imgs = self._get_variable(label_imgs.type(torch.LongTensor))\n","\n","    #         self.optim.zero_grad()\n","    #         pred_map, softmax, bias_fork = self.net(images)\n","\n","\n","    #         loss = self.loss(pred_map, torch.squeeze(label_imgs)) #again not sure about squeeze check deeplab train script. \n","            \n","    #         batch_size = images.shape[0]\n","    #         total_num_correct += self._num_correct(pred_map, label_imgs, topk=1).data[0]\n","    #         total_loss += loss.data[0]*batch_size\n","    #         total_num_test += batch_size\n","               \n","    #     avg_loss = total_loss/total_num_test\n","    #     avg_acc = total_num_correct/total_num_test\n","    #     msg = \"EVALUATION LOSS  %.4f, ACCURACY : %.4f (%d/%d)\" % \\\n","    #                     (avg_loss,avg_acc,int(total_num_correct),total_num_test)\n","    #     self.logger.info(msg)\n","\n","\n","    def _num_correct(self,outputs,labels,topk=1):\n","        _, preds = outputs.topk(k=topk, dim=1)\n","        preds = preds.t()\n","        correct = preds.eq(labels.view(1, -1).expand_as(preds))\n","        correct = correct.view(-1).sum()\n","        return correct\n","        \n","\n","    def _accuracy(self, outputs, labels):\n","        batch_size = labels.size(0)\n","        _, preds = outputs.topk(k=1, dim=1)\n","        preds = preds.t()\n","        correct = preds.eq(labels.view(1, -1).expand_as(preds))\n","        correct = correct.view(-1).float().sum(0, keepdim=True)\n","        accuracy = correct.mul_(100.0 / batch_size)\n","        return accuracy\n","\n","\n","    def _save_model(self, step): # this requires the directory to already be created (this is done in backend setting in main script)\n","        checkpoint_dir = self.path + '/checkpoints'\n","        if not os.path.exists(checkpoint_dir):\n","            os.makedirs(checkpoint_dir)\n","\n","        torch.save({\n","            'step': step,\n","            'optim_state_dict': self.optim.state_dict(),\n","            'net_state_dict': self.net.state_dict()\n","        }, os.path.join(checkpoint_dir, 'checkpoint_epoch_%04d.pth' % (step+1)) )\n","        print('Checkpoint saved. Epoch : %d'%step)\n","\n","\n","    def _load_model(self):\n","        ckpt = torch.load(self.option.checkpoint)\n","        self.net.load_state_dict(ckpt['net_state_dict']) # this is how we saved them in save model method above\n","        self.optim.load_state_dict(ckpt['optim_state_dict'])\n","\n","    def _plotter_function(self, epoch_losses, stage, head = 'seg' ):\n","        if stage == 'train':\n","           if head == 'seg':\n","                with open(\"%s/epoch_losses_train.pkl\" % self.path, \"wb\") as file:\n","                    pickle.dump(epoch_losses, file)\n","                plt.figure(1)\n","                plt.plot(epoch_losses, \"k^\")\n","                plt.plot(epoch_losses, \"k\")\n","                plt.ylabel(\"loss\")\n","                plt.xlabel(\"epoch\")\n","                plt.title(\"train loss per epoch\")\n","                plt.savefig(\"%s/epoch_losses_train.png\" % self.path )\n","                plt.close(1)\n","           elif head == 'bias':\n","                with open(\"%s/epoch_losses_train_bias_head.pkl\" % self.path, \"wb\") as file:\n","                    pickle.dump(epoch_losses, file)\n","                plt.figure(1)\n","                plt.plot(epoch_losses, \"m^\")\n","                plt.plot(epoch_losses, \"m\")\n","                plt.ylabel(\"loss\")\n","                plt.xlabel(\"epoch\")\n","                plt.title(\"train loss per epoch in bias head\")\n","                plt.savefig(\"%s/epoch_losses_train_bias_head.png\" % self.path )\n","                plt.close(1)\n","        \n","        elif stage == 'val':\n","           if head == 'seg':\n","                with open(\"%s/epoch_losses_val.pkl\" % self.path, \"wb\") as file:\n","                    pickle.dump(epoch_losses, file)\n","                plt.figure(1)\n","                plt.plot(epoch_losses, \"k^\")\n","                plt.plot(epoch_losses, \"k\")\n","                plt.ylabel(\"loss\")\n","                plt.xlabel(\"epoch\")\n","                plt.title(\"val loss per epoch\")\n","                plt.savefig(\"%s/epoch_losses_val.png\" % self.path )\n","                plt.close(1)\n","           elif head == 'bias':\n","                with open(\"%s/epoch_losses_val_bias_head.pkl\" % self.path, \"wb\") as file:\n","                    pickle.dump(epoch_losses, file)\n","                plt.figure(1)\n","                plt.plot(epoch_losses, \"m^\")\n","                plt.plot(epoch_losses, \"m\")\n","                plt.ylabel(\"loss\")\n","                plt.xlabel(\"epoch\")\n","                plt.title(\"val loss per epoch in bias head\")\n","                plt.savefig(\"%s/epoch_losses_val_bias_head.png\" % self.path )\n","                plt.close(1)\n","\n","    def train(self, train_loader, val_loader=None):\n","        #self._initialization()\n","        #if self.option.checkpoint is not None:\n","        #    self._load_model()\n","\n","        self._mode_setting(is_train=True) #sets the model in .train mode\n","        timer = Timer(self.logger, self.option.max_step)\n","        start_epoch = 0\n","        \n","        epoch_losses_train = []\n","        epoch_losses_val = []\n","        epoch_losses_train_seg = []\n","        epoch_losses_val_seg = []\n","        epoch_losses_train_bias = []\n","        epoch_losses_val_bias = []\n","        \n","        for step in range(start_epoch, self.option.max_step):\n","            if self.option.train_baseline:\n","                batch_losses = self._train_step_baseline(train_loader, step)\n","                epoch_loss = np.mean(batch_losses)\n","                print('Training epoch loss:....', epoch_loss)\n","                epoch_losses_train.append(epoch_loss)\n","                self._plotter_function(epoch_losses_train, stage = 'train')\n","                ######\n","                batch_losses = self._validate_step_baseline(val_loader, step)\n","                epoch_loss = np.mean(batch_losses)\n","                print('Validation epoch loss:....', epoch_loss)\n","                epoch_losses_val.append(epoch_loss)\n","                epoch_losses = self._plotter_function(epoch_losses_val, stage = 'val')\n","            else:\n","                print('epoch....', step)\n","                batch_losses_seg, batch_losses_bias = self._train_step(train_loader,step) #LNTL proceedure\n","                epoch_loss_seg = np.mean(batch_losses_seg)\n","                epoch_loss_bias = np.mean(batch_losses_bias)\n","                print('Training epoch loss (segmentation head / bias head):....', epoch_loss_seg, '  /  ' , epoch_loss_bias)\n","                epoch_losses_train_seg.append(epoch_loss_seg)\n","                epoch_losses_train_bias.append(epoch_loss_bias)\n","                # Plotting and save to pickle\n","                self._plotter_function(epoch_losses_train_seg, stage = 'train')\n","                self._plotter_function(epoch_losses_train_bias, stage= 'train', head = 'bias')\n","                ######\n","                batch_losses_seg, batch_losses_bias = self._validate_step(val_loader, step)\n","                epoch_loss_seg = np.mean(batch_losses_seg)\n","                epoch_loss_bias = np.mean(batch_losses_bias)\n","                print('Validation epoch loss (segmentation head / bias head):....', epoch_loss_seg, '  /  ' , epoch_loss_bias)\n","                epoch_losses_val_seg.append(epoch_loss_seg)\n","                epoch_losses_val_bias.append(epoch_loss_bias)\n","                # Plotting and save to pickle\n","                self._plotter_function(epoch_losses_val_seg, stage = 'val')\n","                self._plotter_function(epoch_losses_val_bias, stage = 'val', head = 'bias')\n","            \n","            self.scheduler.step()\n","            self.scheduler_r.step()\n","            self.scheduler_g.step()\n","            self.scheduler_b.step()\n","\n","            if step == 1 or step % self.option.save_step == 0 or step == (self.option.max_step-1):\n","                self._save_model(step)\n","\n","\n","    def _get_variable(self, inputs):\n","        if self.option.cuda:\n","            return Variable(inputs.cuda()) #Is there a difference between Variable(inputs).cuda()??\n","        return Variable(inputs)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: import_ipynb in /usr/local/lib/python3.7/dist-packages (0.1.3)\n","importing Jupyter notebook from /content/drive/My Drive/BiasMitigation/LNTL/models/SegNet/segnet.ipynb\n","importing Jupyter notebook from /content/drive/My Drive/BiasMitigation/LNTL/models/BiasPredictor.ipynb\n","importing Jupyter notebook from /content/drive/My Drive/BiasMitigation/LNTL/utils/utils_LNTL.ipynb\n"],"name":"stdout"}]}]}