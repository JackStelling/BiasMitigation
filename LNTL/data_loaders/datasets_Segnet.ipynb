{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"datasets.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"PgsMV8oailzt"},"source":["import torch\n","import torch.utils.data\n","\n","import numpy as np\n","import cv2\n","import os\n","\n","train_dirs = [\"jena/\", \"zurich/\", \"weimar/\", \"ulm/\", \"tubingen/\", \"stuttgart/\",\n","              \"strasbourg/\", \"monchengladbach/\", \"krefeld/\", \"hanover/\",\n","              \"hamburg/\", \"erfurt/\", \"dusseldorf/\", \"darmstadt/\", \"cologne/\",\n","              \"bremen/\", \"bochum/\", \"aachen/\"]\n","val_dirs = [\"frankfurt/\", \"munster/\", \"lindau/\"]\n","test_dirs = [\"berlin\", \"bielefeld\", \"bonn\", \"leverkusen\", \"mainz\", \"munich\"]\n","\n","class DatasetTrain(torch.utils.data.Dataset):\n","    def __init__(self, cityscapes_data_path, cityscapes_meta_path):\n","        self.img_dir = cityscapes_data_path + \"/leftImg8bit/train/\"\n","        self.label_dir = cityscapes_meta_path + \"/label_imgs/\"\n","\n","        self.img_h = 1024\n","        self.img_w = 2048\n","\n","        self.new_img_h = 512\n","        self.new_img_w = 1024\n","\n","        self.examples = []\n","        for train_dir in train_dirs:\n","            train_img_dir_path = self.img_dir + train_dir\n","\n","            file_names = os.listdir(train_img_dir_path)\n","            for file_name in file_names:\n","                img_id = file_name.split(\"_leftImg8bit.png\")[0]\n","\n","                img_path = train_img_dir_path + file_name\n","\n","                label_img_path = self.label_dir + img_id + \".png\"\n","\n","                example = {}\n","                example[\"img_path\"] = img_path\n","                example[\"label_img_path\"] = label_img_path\n","                example[\"img_id\"] = img_id\n","                self.examples.append(example)\n","\n","        self.num_examples = len(self.examples)\n","\n","    def __getitem__(self, index):\n","        example = self.examples[index]\n","\n","        img_path = example[\"img_path\"]\n","        img = cv2.imread(img_path, -1) # (shape: (1024, 2048, 3))\n","        # resize img without interpolation (want the image to still match\n","        # label_img, which we resize below):\n","        img = cv2.resize(img, (self.new_img_w, self.new_img_h),\n","                         interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024, 3))\n","\n","        label_img_path = example[\"label_img_path\"]\n","        label_img = cv2.imread(label_img_path, -1) # (shape: (1024, 2048))\n","        # resize label_img without interpolation (want the resulting image to\n","        # still only contain pixel values corresponding to an object class):\n","        label_img = cv2.resize(label_img, (self.new_img_w, self.new_img_h),\n","                               interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024))\n","\n","        # flip the img and the label with 0.5 probability:\n","        flip = np.random.randint(low=0, high=2)\n","        if flip == 1:\n","            img = cv2.flip(img, 1)\n","            label_img = cv2.flip(label_img, 1)\n","\n","        ########################################################################\n","        # randomly scale the img and the label:\n","        ########################################################################\n","        scale = np.random.uniform(low=0.7, high=2.0)\n","        new_img_h = int(scale*self.new_img_h)\n","        new_img_w = int(scale*self.new_img_w)\n","\n","        # resize img without interpolation (want the image to still match\n","        # label_img, which we resize below):\n","        img = cv2.resize(img, (new_img_w, new_img_h),\n","                         interpolation=cv2.INTER_NEAREST) # (shape: (new_img_h, new_img_w, 3))\n","\n","        # resize label_img without interpolation (want the resulting image to\n","        # still only contain pixel values corresponding to an object class):\n","        label_img = cv2.resize(label_img, (new_img_w, new_img_h),\n","                               interpolation=cv2.INTER_NEAREST) # (shape: (new_img_h, new_img_w))\n","        ########################################################################\n","\n","        # # # # # # # # debug visualization START\n","        # print (scale)\n","        # print (new_img_h)\n","        # print (new_img_w)\n","        #\n","        # cv2.imshow(\"test\", img)\n","        # cv2.waitKey(0)\n","        #\n","        # cv2.imshow(\"test\", label_img)\n","        # cv2.waitKey(0)\n","        # # # # # # # # debug visualization END\n","\n","        ########################################################################\n","        # select a 256x256 random crop from the img and label:\n","        ########################################################################\n","        start_x = np.random.randint(low=0, high=(new_img_w - 256))\n","        end_x = start_x + 256\n","        start_y = np.random.randint(low=0, high=(new_img_h - 256))\n","        end_y = start_y + 256\n","\n","        img = img[start_y:end_y, start_x:end_x] # (shape: (256, 256, 3))\n","        label_img = label_img[start_y:end_y, start_x:end_x] # (shape: (256, 256))\n","        ########################################################################\n","\n","        # # # # # # # # debug visualization START\n","        # print (img.shape)\n","        # print (label_img.shape)\n","        #\n","        # cv2.imshow(\"test\", img)\n","        # cv2.waitKey(0)\n","        #\n","        # cv2.imshow(\"test\", label_img)\n","        # cv2.waitKey(0)\n","        # # # # # # # # debug visualization END\n","\n","        # normalize the img (with the mean and std for the pretrained ResNet):\n","        img = img/255.0\n","        img = img - np.array([0.485, 0.456, 0.406])\n","        img = img/np.array([0.229, 0.224, 0.225]) # (shape: (256, 256, 3))\n","        img = np.transpose(img, (2, 0, 1)) # (shape: (3, 256, 256))\n","        img = img.astype(np.float32)\n","\n","        # convert numpy -> torch:\n","        img = torch.from_numpy(img) # (shape: (3, 256, 256))\n","        label_img = torch.from_numpy(label_img) # (shape: (256, 256))\n","\n","        return (img, label_img)\n","\n","    def __len__(self):\n","        return self.num_examples\n","\n","class DatasetVal(torch.utils.data.Dataset):\n","    def __init__(self, cityscapes_data_path, cityscapes_meta_path):\n","        self.img_dir = cityscapes_data_path + \"/leftImg8bit/val/\"\n","        self.label_dir = cityscapes_meta_path + \"/label_imgs/\"\n","\n","        self.img_h = 1024\n","        self.img_w = 2048\n","\n","        self.new_img_h = 512\n","        self.new_img_w = 1024\n","\n","        self.examples = []\n","        for val_dir in val_dirs:\n","            val_img_dir_path = self.img_dir + val_dir\n","\n","            file_names = os.listdir(val_img_dir_path)\n","            for file_name in file_names:\n","                img_id = file_name.split(\"_leftImg8bit.png\")[0]\n","\n","                img_path = val_img_dir_path + file_name\n","\n","                label_img_path = self.label_dir + img_id + \".png\"\n","                label_img = cv2.imread(label_img_path, -1) # (shape: (1024, 2048))\n","\n","                example = {}\n","                example[\"img_path\"] = img_path\n","                example[\"label_img_path\"] = label_img_path\n","                example[\"img_id\"] = img_id\n","                self.examples.append(example)\n","\n","        self.num_examples = len(self.examples)\n","\n","    def __getitem__(self, index):\n","        example = self.examples[index]\n","\n","        img_id = example[\"img_id\"]\n","\n","        img_path = example[\"img_path\"]\n","        img = cv2.imread(img_path, -1) # (shape: (1024, 2048, 3))\n","        # resize img without interpolation (want the image to still match\n","        # label_img, which we resize below):\n","        img = cv2.resize(img, (self.new_img_w, self.new_img_h),\n","                         interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024, 3))\n","\n","        label_img_path = example[\"label_img_path\"]\n","        label_img = cv2.imread(label_img_path, -1) # (shape: (1024, 2048))\n","        # resize label_img without interpolation (want the resulting image to\n","        # still only contain pixel values corresponding to an object class):\n","        label_img = cv2.resize(label_img, (self.new_img_w, self.new_img_h),\n","                               interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024))\n","\n","        # # # # # # # # debug visualization START\n","        # cv2.imshow(\"test\", img)\n","        # cv2.waitKey(0)\n","        #\n","        # cv2.imshow(\"test\", label_img)\n","        # cv2.waitKey(0)\n","        # # # # # # # # debug visualization END\n","\n","        # normalize the img (with the mean and std for the pretrained ResNet):\n","        img = img/255.0\n","        img = img - np.array([0.485, 0.456, 0.406])\n","        img = img/np.array([0.229, 0.224, 0.225]) # (shape: (512, 1024, 3))\n","        img = np.transpose(img, (2, 0, 1)) # (shape: (3, 512, 1024))\n","        img = img.astype(np.float32)\n","\n","        # convert numpy -> torch:\n","        img = torch.from_numpy(img) # (shape: (3, 512, 1024))\n","        label_img = torch.from_numpy(label_img) # (shape: (512, 1024))\n","\n","        return (img, label_img, img_id)\n","\n","    def __len__(self):\n","        return self.num_examples\n","\n","class DatasetSeq(torch.utils.data.Dataset):\n","    def __init__(self, cityscapes_data_path, cityscapes_meta_path, sequence):\n","        self.img_dir = cityscapes_data_path + \"/leftImg8bit/demoVideo/stuttgart_\" + sequence + \"/\"\n","\n","        self.img_h = 1024\n","        self.img_w = 2048\n","\n","        self.new_img_h = 512\n","        self.new_img_w = 1024\n","\n","        self.examples = []\n","\n","        file_names = os.listdir(self.img_dir)\n","        for file_name in file_names:\n","            img_id = file_name.split(\"_leftImg8bit.png\")[0]\n","\n","            img_path = self.img_dir + file_name\n","\n","            example = {}\n","            example[\"img_path\"] = img_path\n","            example[\"img_id\"] = img_id\n","            self.examples.append(example)\n","\n","        self.num_examples = len(self.examples)\n","\n","    def __getitem__(self, index):\n","        example = self.examples[index]\n","\n","        img_id = example[\"img_id\"]\n","\n","        img_path = example[\"img_path\"]\n","        img = cv2.imread(img_path, -1) # (shape: (1024, 2048, 3))\n","        # resize img without interpolation:\n","        img = cv2.resize(img, (self.new_img_w, self.new_img_h),\n","                         interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024, 3))\n","\n","        # normalize the img (with the mean and std for the pretrained ResNet):\n","        img = img/255.0\n","        img = img - np.array([0.485, 0.456, 0.406])\n","        img = img/np.array([0.229, 0.224, 0.225]) # (shape: (512, 1024, 3))\n","        img = np.transpose(img, (2, 0, 1)) # (shape: (3, 512, 1024))\n","        img = img.astype(np.float32)\n","\n","        # convert numpy -> torch:\n","        img = torch.from_numpy(img) # (shape: (3, 512, 1024))\n","\n","        return (img, img_id)\n","\n","    def __len__(self):\n","        return self.num_examples\n","\n","class DatasetThnSeq(torch.utils.data.Dataset):\n","    def __init__(self, thn_data_path):\n","        self.img_dir = thn_data_path + \"/\"\n","\n","        self.examples = []\n","\n","        file_names = os.listdir(self.img_dir)\n","        for file_name in file_names:\n","            img_id = file_name.split(\".png\")[0]\n","\n","            img_path = self.img_dir + file_name\n","\n","            example = {}\n","            example[\"img_path\"] = img_path\n","            example[\"img_id\"] = img_id\n","            self.examples.append(example)\n","\n","        self.num_examples = len(self.examples)\n","\n","    def __getitem__(self, index):\n","        example = self.examples[index]\n","\n","        img_id = example[\"img_id\"]\n","\n","        img_path = example[\"img_path\"]\n","        img = cv2.imread(img_path, -1) # (shape: (512, 1024, 3))\n","\n","        # normalize the img (with mean and std for the pretrained ResNet):\n","        img = img/255.0\n","        img = img - np.array([0.485, 0.456, 0.406])\n","        img = img/np.array([0.229, 0.224, 0.225]) # (shape: (512, 1024, 3))\n","        img = np.transpose(img, (2, 0, 1)) # (shape: (3, 512, 1024))\n","        img = img.astype(np.float32)\n","\n","        # convert numpy -> torch:\n","        img = torch.from_numpy(img) # (shape: (3, 512, 1024))\n","\n","        return (img, img_id)\n","\n","    def __len__(self):\n","        return self.num_examples"],"execution_count":null,"outputs":[]}]}