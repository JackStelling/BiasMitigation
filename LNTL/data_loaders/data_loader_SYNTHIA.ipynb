{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"data_loader_SYNTHIA.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"5xaPi15VJ__F"},"source":["import torch\n","import torch.utils.data\n","import torchvision.transforms as T\n","\n","import numpy as np\n","import cv2\n","import os\n","\n","\n","class DatasetTrain_SYNTHIA(torch.utils.data.Dataset):\n","    def __init__(self, option): # could add self.option.data_path and self.option.metadata_path\n","\n","        self.option = option\n","\n","        self.img_dir = '/content/drive/MyDrive/BiasMitigation/Datasets/SYNTHIA/RGB'\n","        self.label_dir = '/content/drive/MyDrive/BiasMitigation/Datasets/SYNTHIA/GT/SEMANTIC_LABELS'\n","\n","        self.img_h = 760   \n","        self.img_w = 1280\n","\n","        self.new_img_h = 512\n","        self.new_img_w = 1024\n","\n","        train_img_dir_path = self.img_dir + \"/train\"\n","        file_names = os.listdir(train_img_dir_path)\n","        self.examples = []\n","\n","        for file_name in file_names:\n","            img_path = train_img_dir_path + '/' + file_name\n","            label_img_path = self.label_dir + '/train/' + file_name\n","\n","            example = {}\n","            example[\"img_path\"] = img_path\n","            example[\"label_img_path\"] = label_img_path\n","            example[\"img_id\"] = file_name \n","            self.examples.append(example)\n","\n","        self.num_examples = len(self.examples)\n","\n","\n","    def __getitem__(self, index):\n","        example = self.examples[index]\n","        greyscale = self.option.train_greyscale \n","\n","        img_path = example[\"img_path\"]\n","        \n","        if greyscale: \n","            grey_img =  cv2.imread(img_path, 0) # (shape: (760, 1280))\n","            dummy_RGB_image = np.ndarray(shape=(grey_img.shape[0], grey_img.shape[1], 3), dtype= np.uint8) \n","            dummy_RGB_image[:, :, 0] = grey_img[:, :]\n","            dummy_RGB_image[:, :, 1] = grey_img[:, :]\n","            dummy_RGB_image[:, :, 2] = grey_img[:, :]\n","            img = dummy_RGB_image  # (shape: (760, 1280, 3)) grey values copied along all 3 channels for use in pretrained networks expecting 3 channel input\n","        else:\n","            img = cv2.imread(img_path, -1) # (shape: (760, 1280, 3))\n","        \n","        # resize img without interpolation (want the image to still match\n","        # label_img, which we resize below):\n","        img = cv2.resize(img, (self.new_img_w, self.new_img_h),\n","                         interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024, 3))\n","\n","        label_img_path = example[\"label_img_path\"]\n","        label_img = cv2.imread(label_img_path, -1) # (shape: (760, 1280))\n","        # resize label_img without interpolation (want the resulting image to\n","        # still only contain pixel values corresponding to an object class):\n","        label_img = cv2.resize(label_img, (self.new_img_w, self.new_img_h),\n","                               interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024))\n","\n","\n","        #### IMAGE AUGMENTATION #### - This must be completed for labels, bias labels and raw images ** UPDATE changed bias label to extract after img aug steps **\n","        # flip the img and the label with 0.5 probability:\n","        flip = np.random.randint(low=0, high=2)\n","        if flip == 1:\n","            img = cv2.flip(img, 1)\n","            label_img = cv2.flip(label_img, 1)\n","            #bias_label = cv2.flip(bias_label, 1)\n","\n","        ########################################################################\n","        # randomly scale the img and the label:\n","        ########################################################################\n","        scale = np.random.uniform(low=0.7, high=2.0)\n","        new_img_h = int(scale*self.new_img_h)\n","        new_img_w = int(scale*self.new_img_w)\n","\n","        # resize img without interpolation (want the image to still match\n","        # label_img, which we resize below):\n","        img = cv2.resize(img, (new_img_w, new_img_h),\n","                         interpolation=cv2.INTER_NEAREST) # (shape: (new_img_h, new_img_w, 3))\n","\n","        # resize label_img without interpolation (want the resulting image to\n","        # still only contain pixel values corresponding to an object class):\n","        label_img = cv2.resize(label_img, (new_img_w, new_img_h),\n","                               interpolation=cv2.INTER_NEAREST) # (shape: (new_img_h, new_img_w))\n","\n","        #bias_label = cv2.resize(bias_label, (new_img_w, new_img_h),\n","        #                       interpolation=cv2.INTER_NEAREST) # (shape: (new_img_h, new_img_w))\n","        \n","        ########################################################################\n","\n","        # # # # # # # # debug visualization START\n","        # print (scale)\n","        # print (new_img_h)\n","        # print (new_img_w)\n","        #\n","        # cv2.imshow(\"test\", img)\n","        # cv2.waitKey(0)\n","        #\n","        # cv2.imshow(\"test\", label_img)\n","        # cv2.waitKey(0)\n","        # # # # # # # # debug visualization END\n","\n","        ########################################################################\n","        # select a 256x256 random crop from the img and label:\n","        ########################################################################\n","        start_x = np.random.randint(low=0, high=(new_img_w - 256))\n","        end_x = start_x + 256\n","        start_y = np.random.randint(low=0, high=(new_img_h - 256))\n","        end_y = start_y + 256\n","\n","        img = img[start_y:end_y, start_x:end_x] # (shape: (256, 256, 3))\n","        label_img = label_img[start_y:end_y, start_x:end_x] # (shape: (256, 256))\n","      \n","        ########################################################################\n","        #~~ At this stage all images enter the model as 256 x 256\n","\n","        # # # # # # # # debug visualization START\n","        # print (img.shape)\n","        # print (label_img.shape)\n","        #\n","        # cv2_imshow(img)\n","        # cv2.waitKey(0)\n","        #\n","        # cv2_imshow(label_img)\n","        # cv2.waitKey(0)\n","        # # # # # # # # debug visualization END\n","\n","        ## Getting bias labels...\n","\n","        bias_label = img\n","        bias_label = cv2.resize(bias_label, (32, 32), # 32x32 becuase we're using resnet 8\n","                               interpolation=cv2.INTER_NEAREST) # Hard code for deeplab size for now...see note below. \n","        ''' could have something here like\n","        if option.network_type == SegNet:\n","            feat_map_height = 256/.. , feat_map_width = 256/ ..\n","        elif option.network_type == Deeplab:\n","            feat_map_height = 256/8 , feat_map_width = 256/8 # using resnet OS8\n","        else:\n","            print('Warning no information for expected feature map size of this Network')\n","        \n","        bias_label = cv2.resize(bias_label, (feat_map_height, feat_map_width),\n","                               interpolation=cv2.INTER_NEAREST)\n","        '''\n","        bias_label = torch.from_numpy(np.transpose(bias_label,(2, 0, 1)))\n","        #mask_image = torch.lt(bias_label.float()-0.00001, 0.) * 255   # Useful if any negative numbers appear\n","        bias_label = torch.div(bias_label,32)  # Change the division here for higher resolution. Divide by 16 would give us 256/16 = 16 bins instead of 256/32 = 8 bins \n","        #bias_label = bias_label + mask_image\n","        bias_label = bias_label.long()\n","        #~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","        # normalize the img (with the mean and std for the pretrained ResNet):\n","        img = img/255.0\n","        img = img - np.array([0.485, 0.456, 0.406])\n","        img = img/np.array([0.229, 0.224, 0.225]) # (shape: (256, 256, 3))\n","        img = np.transpose(img, (2, 0, 1)) # (shape: (3, 256, 256))\n","        img = img.astype(np.float32)\n","\n","        # convert numpy -> torch:\n","        img = torch.from_numpy(img) # (shape: (3, 256, 256))\n","        label_img = torch.from_numpy(label_img) # (shape: (256, 256))\n","        \n","        return (img, bias_label, label_img)\n","\n","\n","    def __len__(self):\n","        return self.num_examples\n","\n","\n","class DatasetVal_SYNTHIA(torch.utils.data.Dataset):\n","    def __init__(self, option):\n","        \n","        self.option = option\n","        self.network = option.network_type\n","\n","        self.img_dir = '/content/drive/MyDrive/BiasMitigation/Datasets/SYNTHIA/RGB'\n","        self.label_dir = '/content/drive/MyDrive/BiasMitigation/Datasets/SYNTHIA/GT/LABELS'\n","\n","        self.img_h = 760   \n","        self.img_w = 1280\n","\n","        self.new_img_h = 512\n","        self.new_img_w = 1024\n","\n","        val_img_dir_path = self.img_dir + \"/val\"\n","        file_names = os.listdir(val_img_dir_path)\n","        self.examples = []\n","\n","        for file_name in file_names:\n","            img_path = val_img_dir_path + '/' + file_name\n","            label_img_path = self.label_dir + '/val/' + file_name\n","\n","            example = {}\n","            example[\"img_path\"] = img_path\n","            example[\"label_img_path\"] = label_img_path\n","            example[\"img_id\"] = file_name \n","            self.examples.append(example)\n","\n","        self.num_examples = len(self.examples)\n","\n","\n","    def __getitem__(self, index):\n","        example = self.examples[index]\n","        greyscale = self.option.train_greyscale\n","        \n","        # These are used for evaluation of the LNTL and baseline methods\n","        val_only_greyscale = self.option.val_only_greyscale\n","        val_only_jitter = self.option.val_only_jitter\n","\n","        img_id = example[\"img_id\"]\n","\n","        img_path = example[\"img_path\"]\n","\n","        if greyscale: # Do we need an option argument to use this? \n","            grey_img =  cv2.imread(img_path, 0) # (shape: (1024, 2048))\n","            dummy_RGB_image = np.ndarray(shape=(grey_img.shape[0], grey_img.shape[1], 3), dtype= np.uint8) \n","            dummy_RGB_image[:, :, 0] = grey_img[:, :]\n","            dummy_RGB_image[:, :, 1] = grey_img[:, :]\n","            dummy_RGB_image[:, :, 2] = grey_img[:, :]\n","            img = dummy_RGB_image  # (shape: (1024, 2048, 3)) grey values copied along all 3 channels for use in pretrained networks expecting 3 channel input\n","        else:\n","            img = cv2.imread(img_path, -1) # (shape: (1024, 2048, 3))\n","\n","\n","        ##### Functions for creating a bias testing image set #####\n","        ##### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #####\n","        if val_only_greyscale: \n","            grey_img =  cv2.imread(img_path, 0) # (shape: (1024, 2048))\n","            dummy_RGB_image = np.ndarray(shape=(grey_img.shape[0], grey_img.shape[1], 3), dtype= np.uint8) \n","            dummy_RGB_image[:, :, 0] = grey_img[:, :]\n","            dummy_RGB_image[:, :, 1] = grey_img[:, :]\n","            dummy_RGB_image[:, :, 2] = grey_img[:, :]\n","            img = dummy_RGB_image  # (shape: (1024, 2048, 3)) grey values copied along all 3 channels for use in pretrained networks expecting 3 channel input\n","        else:\n","            img = cv2.imread(img_path, -1) # (shape: (1024, 2048, 3))\n","\n","        if val_only_jitter:  #need to have al look at jitter settings\n","            img =  cv2.imread(img_path, -1) # (shape: (1024, 2048, 3))\n","            img = torch.from_numpy(np.transpose(img,(2, 0, 1))) # convert to torch tensor to use torch transform, and transpose to (channels, h, w)\n","            jitter_instance = T.ColorJitter( contrast=.9, saturation=.9, hue=.5)  #brightness=.5,\n","            jittered_imgs = jitter_instance(img) \n","            jittered_imgs = jittered_imgs.detach().cpu().numpy() # convert back to numpy array\n","            img = np.transpose(jittered_imgs,(1, 2, 0)) # transpose (h, w, channels) for cv2\n","            cv2_imshow(img)\n","        else:\n","            img = cv2.imread(img_path, -1) # (shape: (1024, 2048, 3))\n","\n","        if val_only_invert: \n","            img =  cv2.imread(img_path, -1) # (shape: (1024, 2048))\n","            img = torch.from_numpy(np.transpose(img,(2, 0, 1))) # convert to torch tensor to use torch transform, and transpose BGR -> RGB\n","            inv_imgs = T.functional.invert(img) \n","            inv_imgs = inv_imgs.detach().cpu().numpy() # convert back to numpy array\n","            img = np.transpose(inv_imgs,(1, 2, 0)) # transpose RGB -> BGR\n","            cv2_imshow(img)\n","        else:\n","            img = cv2.imread(img_path, -1) # (shape: (1024, 2048, 3))\n","\n","        ##### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #####\n","        ##### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #####\n","\n","        # resize img without interpolation (want the image to still match\n","        # label_img, which we resize below):\n","        img = cv2.resize(img, (self.new_img_w, self.new_img_h),\n","                         interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024, 3))\n","\n","        label_img_path = example[\"label_img_path\"]\n","        label_img = cv2.imread(label_img_path, -1) # (shape: (1024, 2048))\n","        # resize label_img without interpolation (want the resulting image to\n","        # still only contain pixel values corresponding to an object class):\n","        label_img = cv2.resize(label_img, (self.new_img_w, self.new_img_h),\n","                               interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024))\n","\n","        # # # # # # # # debug visualization START\n","        # cv2.imshow(\"test\", img)\n","        # cv2.waitKey(0)\n","        #\n","        # cv2.imshow(\"test\", label_img)\n","        # cv2.waitKey(0)\n","        # # # # # # # # debug visualization END\n","\n","\n","        ## Getting bias labels...\n","\n","        # \n","        # bias_label = cv2.resize(bias_label, (256/16, 256/16),\n","        #                        interpolation=cv2.INTER_NEAREST) # Hard code for deeplab size for now...see note below. \n","       \n","\n","        # if network == 'SegNet':\n","        #     feat_map_height = 256/8 , feat_map_width = 256/8 # need updating\n","        # elif network == 'Deeplab':\n","        #     feat_map_height = 256/8 , feat_map_width = 256/8\n","        # else:\n","        #     print('Warning no information for expected feature map size of this Network')\n","        \n","        bias_label = img\n","        bias_label = cv2.resize(bias_label, (int(self.new_img_w/8), int(self.new_img_h/8)),  # by declaring the size this way is will match the input image coming into the bias fork wich is w/8 h/8 for resnet 18os8. \n","                               interpolation=cv2.INTER_NEAREST)\n","\n","\n","        bias_label = torch.from_numpy(np.transpose(bias_label,(2, 0, 1)))\n","        #mask_image = torch.lt(bias_label.float()-0.00001, 0.) * 255   # Useful if any negative numbers appear\n","        bias_label = torch.div(bias_label,32)  # Change the division here for higher resolution. Divide by 16 would give us 256/16 = 16 bins instead of 256/32 = 8 bins \n","        #bias_label = bias_label + mask_image\n","        bias_label = bias_label.long()\n","        #~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","        # normalize the img (with the mean and std for the pretrained ResNet):\n","        img = img/255.0\n","        img = img - np.array([0.485, 0.456, 0.406])\n","        img = img/np.array([0.229, 0.224, 0.225]) # (shape: (512, 1024, 3))\n","        img = np.transpose(img, (2, 0, 1)) # (shape: (3, 512, 1024))\n","        img = img.astype(np.float32)\n","\n","        # convert numpy -> torch:\n","        img = torch.from_numpy(img) # (shape: (3, 512, 1024))\n","        label_img = torch.from_numpy(label_img) # (shape: (512, 1024))\n","\n","        return (img, bias_label, label_img, img_id)\n","\n","    def __len__(self):\n","        return self.num_examples\n"],"execution_count":null,"outputs":[]}]}