{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"trainer.ipynb","provenance":[],"authorship_tag":"ABX9TyMIEMuAWXTVI8vZZsYgxWfH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"7Xoqp4k5_cmt"},"source":["# -*- coding: utf-8 -*-\n","import torch\n","from torch import nn\n","from torch import optim\n","from torch.autograd import Variable\n","import models\n","\n","import time\n","import os\n","import math\n","\n","from utils import logger_setting, Timer\n","\n","\n","class GradReverse(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, x):\n","        return x.view_as(x)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        return grad_output.neg() * 0.1\n","\n","def grad_reverse(x):\n","    return GradReverse.apply(x)\n","\n","\n","\n","class Trainer(object):\n","    def __init__(self, option):\n","        self.option = option\n","\n","        self._build_model()\n","        self._set_optimizer()\n","        self.logger = logger_setting(option.exp_name, option.save_dir, option.debug)\n","\n","    def _build_model(self):\n","        self.n_color_cls = 8\n","\n","        self.net = models.convnet(num_classes=self.option.n_class)\n","        self.pred_net_r = models.Predictor(input_ch=32, num_classes=self.n_color_cls)\n","        self.pred_net_g = models.Predictor(input_ch=32, num_classes=self.n_color_cls)\n","        self.pred_net_b = models.Predictor(input_ch=32, num_classes=self.n_color_cls)\n","\n","        self.loss = nn.CrossEntropyLoss(ignore_index=255)\n","        self.color_loss = nn.CrossEntropyLoss(ignore_index=255)\n","\n","\n","        if self.option.cuda:\n","            self.net.cuda()\n","            self.pred_net_r.cuda()\n","            self.pred_net_g.cuda()\n","            self.pred_net_b.cuda()\n","            self.loss.cuda()\n","            self.color_loss.cuda()\n","\n","    def _set_optimizer(self):\n","        self.optim = optim.SGD(filter(lambda p: p.requires_grad, self.net.parameters()), lr=self.option.lr, momentum=self.option.momentum, weight_decay=self.option.weight_decay)\n","        self.optim_r = optim.SGD(self.pred_net_r.parameters(), lr=self.option.lr, momentum=self.option.momentum, weight_decay=self.option.weight_decay)\n","        self.optim_g = optim.SGD(self.pred_net_g.parameters(), lr=self.option.lr, momentum=self.option.momentum, weight_decay=self.option.weight_decay)\n","        self.optim_b = optim.SGD(self.pred_net_b.parameters(), lr=self.option.lr, momentum=self.option.momentum, weight_decay=self.option.weight_decay)\n","\n","        #TODO: last_epoch should be the last step of loaded model\n","        lr_lambda = lambda step: self.option.lr_decay_rate ** (step // self.option.lr_decay_period)\n","        self.scheduler = optim.lr_scheduler.LambdaLR(self.optim, lr_lambda=lr_lambda, last_epoch=-1)\n","        self.scheduler_r = optim.lr_scheduler.LambdaLR(self.optim_r, lr_lambda=lr_lambda, last_epoch=-1)\n","        self.scheduler_g = optim.lr_scheduler.LambdaLR(self.optim_g, lr_lambda=lr_lambda, last_epoch=-1)\n","        self.scheduler_b = optim.lr_scheduler.LambdaLR(self.optim_b, lr_lambda=lr_lambda, last_epoch=-1)\n","\n","    @staticmethod\n","    def _weights_init(m):\n","        classname = m.__class__.__name__\n","        if classname.find('Conv') != -1:\n","            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            m.weight.data.normal_(0, math.sqrt(2. / n))\n","        elif classname.find('BatchNorm') != -1:\n","            m.weight.data.fill_(1.0)\n","            m.bias.data.zero_()\n","\n","    def _initialization(self):\n","        self.net.apply(self._weights_init)\n","\n","\n","        if self.option.is_train and self.option.use_pretrain:\n","            if self.option.checkpoint is not None:\n","                self._load_model()\n","            else:\n","                print(\"Pre-trained model not provided\")\n","\n","\n","\n","    def _mode_setting(self, is_train=True):\n","        if is_train:\n","            self.net.train()\n","            self.pred_net_r.train()\n","            self.pred_net_g.train()\n","            self.pred_net_b.train()\n","        else:\n","            self.net.eval()\n","            self.pred_net_r.eval()\n","            self.pred_net_g.eval()\n","            self.pred_net_b.eval()\n","\n","\n","\n","    def _train_step(self, data_loader, step):\n","        _lambda = 0.01\n","\n","        for i, (images,color_labels,labels) in enumerate(data_loader):\n","            \n","            images = self._get_variable(images)\n","            color_labels = self._get_variable(color_labels)\n","            labels = self._get_variable(labels)\n","\n","            self.optim.zero_grad()\n","            self.optim_r.zero_grad()\n","            self.optim_g.zero_grad()\n","            self.optim_b.zero_grad()\n","            feat_label, pred_label = self.net(images)\n","\n","\n","            # predict colors from feat_label. Their prediction should be uniform.\n","            _,pseudo_pred_r = self.pred_net_r(feat_label)\n","            _,pseudo_pred_g = self.pred_net_g(feat_label)\n","            _,pseudo_pred_b = self.pred_net_b(feat_label)\n","\n","\n","            # loss for self.net\n","            loss_pred = self.loss(pred_label, torch.squeeze(labels))\n","\n","            loss_pseudo_pred_r = torch.mean(torch.sum(pseudo_pred_r*torch.log(pseudo_pred_r),1))\n","            loss_pseudo_pred_g = torch.mean(torch.sum(pseudo_pred_g*torch.log(pseudo_pred_g),1))\n","            loss_pseudo_pred_b = torch.mean(torch.sum(pseudo_pred_b*torch.log(pseudo_pred_b),1))\n","\n","            loss_pred_ps_color = (loss_pseudo_pred_r + loss_pseudo_pred_g + loss_pseudo_pred_b) / 3.\n","\n","            loss = loss_pred + loss_pred_ps_color*_lambda\n","\n","            loss.backward()\n","            self.optim.step()\n","\n","\n","            self.optim.zero_grad()\n","            self.optim_r.zero_grad()\n","            self.optim_g.zero_grad()\n","            self.optim_b.zero_grad()\n","\n","            feat_label, pred_label = self.net(images)\n","\n","            feat_color = grad_reverse(feat_label)\n","            pred_r,_ = self.pred_net_r(feat_color)\n","            pred_g,_ = self.pred_net_g(feat_color)\n","            pred_b,_ = self.pred_net_b(feat_color)\n","\n","            # loss for rgb predictors\n","            loss_pred_r = self.color_loss(pred_r, color_labels[:,0])\n","            loss_pred_g = self.color_loss(pred_g, color_labels[:,1])\n","            loss_pred_b = self.color_loss(pred_b, color_labels[:,2])\n","\n","            loss_pred_color = loss_pred_r + loss_pred_g + loss_pred_b\n","\n","            loss_pred_color.backward()\n","            self.optim.step()\n","            self.optim_r.step()\n","            self.optim_g.step()\n","            self.optim_b.step()\n","\n","            if i % self.option.log_step == 0:\n","                msg = \"[TRAIN] cls loss : %.6f, rgb : %.6f, MI : %.6f  (epoch %d.%02d)\" \\\n","                       % (loss_pred,loss_pred_color/3.,loss_pred_ps_color,step,int(100*i/data_loader.__len__()))\n","                self.logger.info(msg)\n","\n","\n","    def _train_step_baseline(self, data_loader, step):\n","        if self.option.greyscale:\n","            \n","\n","            \n","        else:\n","            for i, (images,color_labels,labels) in enumerate(data_loader):\n","                \n","                images = self._get_variable(images)\n","                labels = self._get_variable(labels)\n","\n","                self.optim.zero_grad()\n","                feat_label, pred_label = self.net(images)\n","\n","                # loss for self.net\n","                loss_pred = self.loss(pred_label, torch.squeeze(labels))\n","                loss_pred.backward()\n","                self.optim.step()\n","\n","                # TODO: print elapsed time for iteration\n","                if i % self.option.log_step == 0:\n","                    msg = \"[TRAIN] cls loss : %.6f (epoch %d.%02d)\" \\\n","                        % (loss_pred,step,int(100*i/data_loader.__len__()))\n","                    self.logger.info(msg)\n","\n","\n","\n","\n","    def _validate(self, data_loader):\n","        self._mode_setting(is_train=False)\n","        self._initialization()\n","        if self.option.checkpoint is not None:\n","            self._load_model()\n","        else:\n","            print(\"No trained model for evaluation provided\")\n","            import sys\n","            sys.exit()\n","\n","        num_test = 10000\n","\n","        total_num_correct = 0.\n","        total_num_test = 0.\n","        total_loss = 0.\n","        for i, (images,color_labels,labels) in enumerate(data_loader):\n","            \n","            start_time = time.time()\n","            images = self._get_variable(images)\n","            colro_labels = self._get_variable(color_labels)\n","            labels = self._get_variable(labels)\n","\n","            self.optim.zero_grad()\n","            _, pred_label = self.net(images)\n","\n","\n","            loss = self.loss(pred_label, torch.squeeze(labels))\n","            \n","            batch_size = images.shape[0]\n","            total_num_correct += self._num_correct(pred_label,labels,topk=1).data[0]\n","            total_loss += loss.data[0]*batch_size\n","            total_num_test += batch_size\n","               \n","        avg_loss = total_loss/total_num_test\n","        avg_acc = total_num_correct/total_num_test\n","        msg = \"EVALUATION LOSS  %.4f, ACCURACY : %.4f (%d/%d)\" % \\\n","                        (avg_loss,avg_acc,int(total_num_correct),total_num_test)\n","        self.logger.info(msg)\n","\n","\n","\n","    def _num_correct(self,outputs,labels,topk=1):\n","        _, preds = outputs.topk(k=topk, dim=1)\n","        preds = preds.t()\n","        correct = preds.eq(labels.view(1, -1).expand_as(preds))\n","        correct = correct.view(-1).sum()\n","        return correct\n","        \n","\n","\n","    def _accuracy(self, outputs, labels):\n","        batch_size = labels.size(0)\n","        _, preds = outputs.topk(k=1, dim=1)\n","        preds = preds.t()\n","        correct = preds.eq(labels.view(1, -1).expand_as(preds))\n","        correct = correct.view(-1).float().sum(0, keepdim=True)\n","        accuracy = correct.mul_(100.0 / batch_size)\n","        return accuracy\n","\n","    def _save_model(self, step):\n","        torch.save({\n","            'step': step,\n","            'optim_state_dict': self.optim.state_dict(),\n","            'net_state_dict': self.net.state_dict()\n","        }, os.path.join(self.option.save_dir,self.option.exp_name, 'checkpoint_step_%04d.pth' % step))\n","        print('checkpoint saved. step : %d'%step)\n","\n","    def _load_model(self):\n","        ckpt = torch.load(self.option.checkpoint)\n","        self.net.load_state_dict(ckpt['net_state_dict'])\n","        self.optim.load_state_dict(ckpt['optim_state_dict'])\n","\n","    def train(self, train_loader, val_loader=None):\n","        self._initialization()\n","        if self.option.checkpoint is not None:\n","            self._load_model()\n","\n","        self._mode_setting(is_train=True)\n","        timer = Timer(self.logger, self.option.max_step)\n","        start_epoch = 0\n","        for step in range(start_epoch, self.option.max_step):\n","            if self.option.train_baseline:\n","                self._train_step_baseline(train_loader, step)\n","            else:\n","                self._train_step(train_loader,step)\n","            self.scheduler.step()\n","            self.scheduler_r.step()\n","            self.scheduler_g.step()\n","            self.scheduler_b.step()\n","\n","            if step == 1 or step % self.option.save_step == 0 or step == (self.option.max_step-1):\n","                if val_loader is not None:\n","                    self._validate(step, val_loader)\n","                self._save_model(step)\n","\n","\n","    def _get_variable(self, inputs):\n","        if self.option.cuda:\n","            return Variable(inputs.cuda())\n","        return Variable(inputs)"],"execution_count":null,"outputs":[]}]}