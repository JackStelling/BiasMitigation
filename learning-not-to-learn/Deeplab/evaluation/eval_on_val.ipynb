{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"eval_on_val.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"bkplk8GlS_lj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625231698268,"user_tz":-60,"elapsed":289,"user":{"displayName":"Jack Stelling","photoUrl":"","userId":"10755116166180930038"}},"outputId":"19b0f962-6d3c-4578-d4df-8cc4fdce75b9"},"source":["from google.colab import drive\n","#drive.mount('/content/drive', force_remount=True)\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dwOOFoanDQQC","executionInfo":{"status":"ok","timestamp":1625231704380,"user_tz":-60,"elapsed":354,"user":{"displayName":"Jack Stelling","photoUrl":"","userId":"10755116166180930038"}},"outputId":"d30e5997-cd1f-4057-c3d0-b2102affcd7b"},"source":["import os \n","\n","os.getcwd()\n","\n","os.listdir('/content/drive/MyDrive/Deeplabv3Flat')\n","os.listdir('/content/drive/MyDrive/Datasets')\n","\n","os.chdir('/content/drive/MyDrive/Deeplabv3Flat')\n","print('Directory Changed')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Directory Changed\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eCUc-Zp9SEva","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623334872446,"user_tz":-60,"elapsed":944916,"user":{"displayName":"Jack Stelling","photoUrl":"","userId":"10755116166180930038"}},"outputId":"bc6af8b7-e439-4264-fcd2-d4bf91e0a58a"},"source":["# camera-ready\n","\n","import sys\n","! pip install import-ipynb\n","import import_ipynb\n","\n","sys.path.append(\"/content/drive/MyDrive/Deeplabv3Flat\")\n","from datasets import DatasetVal # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\n","\n","# No need to append with the flat file structure\n","#sys.path.append(\"/root/deeplabv3/model\")\n","from deeplabv3 import DeepLabV3\n","\n","#sys.path.append(\"/content/drive/MyDrive/Deeplabv3Flat\")\n","from utils import label_img_to_color\n","\n","import torch\n","import torch.utils.data\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","import numpy as np\n","import pickle\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","import matplotlib.pyplot as plt\n","import cv2\n","\n","batch_size = 2\n","\n","best_trained_model = 'model_2_epoch_85.pth' # NB put the file name of best weight checkpoint \n","\n","network = DeepLabV3(\"eval_val\", project_dir=\"/content/drive/MyDrive/Deeplabv3Flat\").cuda()\n","network.load_state_dict(torch.load(\"/content/drive/MyDrive/Deeplabv3Flat/training_logs/model_2/checkpoints/\" + best_trained_model))\n","\n","val_dataset = DatasetVal(cityscapes_data_path=\"/content/drive/MyDrive/Datasets/Cityscapes\",\n","                         cityscapes_meta_path=\"/content/drive/MyDrive/Datasets/Cityscapes/meta\")\n","\n","num_val_batches = int(len(val_dataset)/batch_size)\n","print (\"num_val_batches:\", num_val_batches)\n","\n","val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n","                                         batch_size=batch_size, shuffle=False,\n","                                         num_workers=1)\n","\n","with open(\"/content/drive/MyDrive/Datasets/Cityscapes/meta/class_weights.pkl\", \"rb\") as file: # (needed for python3)\n","    class_weights = np.array(pickle.load(file))\n","class_weights = torch.from_numpy(class_weights)\n","class_weights = Variable(class_weights.type(torch.FloatTensor)).cuda()\n","\n","# loss function\n","loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n","\n","network.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n","batch_losses = []\n","for step, (imgs, label_imgs, img_ids) in enumerate(val_loader):\n","    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n","        imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n","        label_imgs = Variable(label_imgs.type(torch.LongTensor)).cuda() # (shape: (batch_size, img_h, img_w))\n","\n","        outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))  # imgs are the raw images from the cityscapes validation folder \n","\n","        # compute the loss:\n","        loss = loss_fn(outputs, label_imgs)\n","        loss_value = loss.data.cpu().numpy()\n","        batch_losses.append(loss_value)\n","\n","        ########################################################################\n","        # save data for visualization:\n","        ########################################################################\n","        outputs = outputs.data.cpu().numpy() # (shape: (batch_size, num_classes, img_h, img_w))\n","        pred_label_imgs = np.argmax(outputs, axis=1) # (shape: (batch_size, img_h, img_w))\n","        pred_label_imgs = pred_label_imgs.astype(np.uint8)\n","\n","        for i in range(pred_label_imgs.shape[0]):\n","            if i == 0:\n","                pred_label_img = pred_label_imgs[i] # (shape: (img_h, img_w))\n","                img_id = img_ids[i]\n","                img = imgs[i] # (shape: (3, img_h, img_w))\n","\n","                img = img.data.cpu().numpy()\n","                img = np.transpose(img, (1, 2, 0)) # (shape: (img_h, img_w, 3))\n","                img = img*np.array([0.229, 0.224, 0.225])\n","                img = img + np.array([0.485, 0.456, 0.406])\n","                img = img*255.0\n","                img = img.astype(np.uint8)\n","\n","                pred_label_img_color = label_img_to_color(pred_label_img)\n","                overlayed_img = 0.35*img + 0.65*pred_label_img_color\n","                overlayed_img = overlayed_img.astype(np.uint8)\n","\n","                cv2.imwrite(network.model_dir + \"/\" + img_id + \"_overlayed.png\", overlayed_img)\n","\n","val_loss = np.mean(batch_losses)\n","print (\"val loss: %g\" % val_loss)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: import-ipynb in /usr/local/lib/python3.7/dist-packages (0.1.3)\n","importing Jupyter notebook from datasets.ipynb\n","importing Jupyter notebook from deeplabv3.ipynb\n","importing Jupyter notebook from resnet.ipynb\n","importing Jupyter notebook from aspp.ipynb\n","importing Jupyter notebook from utils.ipynb\n","pretrained resnet, 18\n","num_val_batches: 250\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3328: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["val loss: 0.538642\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x0NPCfEh7e9I","executionInfo":{"status":"ok","timestamp":1625236905332,"user_tz":-60,"elapsed":12553,"user":{"displayName":"Jack Stelling","photoUrl":"","userId":"10755116166180930038"}},"outputId":"8b6ca393-c31e-4e8e-cc95-bb43d5c9fd7b"},"source":["# visualising the matrix of the output trying to bug test for the colour mismatch from the gt labels. \n","\n","import sys\n","! pip install import-ipynb\n","import import_ipynb\n","\n","sys.path.append(\"/content/drive/MyDrive/Deeplabv3Flat\")\n","from datasets import DatasetVal # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\n","\n","# No need to append with the flat file structure\n","#sys.path.append(\"/root/deeplabv3/model\")\n","from deeplabv3 import DeepLabV3\n","\n","#sys.path.append(\"/content/drive/MyDrive/Deeplabv3Flat\")\n","from utils import label_img_to_color\n","\n","import torch\n","import torch.utils.data\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","import numpy as np\n","import pickle\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","import matplotlib.pyplot as plt\n","import cv2\n","\n","batch_size = 2\n","\n","best_trained_model = 'model_2_epoch_85.pth' # NB put the file name of best weight checkpoint \n","\n","network = DeepLabV3(\"eval_val\", project_dir=\"/content/drive/MyDrive/Deeplabv3Flat\").cuda()\n","network.load_state_dict(torch.load(\"/content/drive/MyDrive/Deeplabv3Flat/training_logs/model_2/checkpoints/\" + best_trained_model))\n","\n","val_dataset = DatasetVal(cityscapes_data_path=\"/content/drive/MyDrive/Datasets/Cityscapes\",\n","                         cityscapes_meta_path=\"/content/drive/MyDrive/Datasets/Cityscapes/meta\")\n","\n","num_val_batches = int(len(val_dataset)/batch_size)\n","print (\"num_val_batches:\", num_val_batches)\n","\n","val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n","                                         batch_size=batch_size, shuffle=False,\n","                                         num_workers=1)\n","\n","with open(\"/content/drive/MyDrive/Datasets/Cityscapes/meta/class_weights.pkl\", \"rb\") as file: # (needed for python3)\n","    class_weights = np.array(pickle.load(file))\n","class_weights = torch.from_numpy(class_weights)\n","class_weights = Variable(class_weights.type(torch.FloatTensor)).cuda()\n","\n","# loss function\n","loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n","\n","trainId_to_id = {\n","    0: 7,\n","    1: 8,\n","    2: 11,\n","    3: 12,\n","    4: 13,\n","    5: 17,\n","    6: 19,\n","    7: 20,\n","    8: 21,\n","    9: 22,\n","    10: 23,\n","    11: 24,\n","    12: 25,\n","    13: 26,\n","    14: 27,\n","    15: 28,\n","    16: 31,\n","    17: 32,\n","    18: 33,\n","    19: 0\n","}\n","trainId_to_id_map_func = np.vectorize(trainId_to_id.get)\n","\n","def label_img_to_color_BGR(img):\n","    label_to_color = {\n","        0:  [128, 64,128],\n","        1:  [232, 35,244],\n","        2:  [ 70, 70, 70],\n","        3:  [156,102,102],\n","        4:  [153,153,190],\n","        5:  [153,153,153],\n","        6:  [30 ,170,250],\n","        7:  [  0,220,220],\n","        8:  [35 ,142,107],\n","        9:  [152,251,152],\n","        10: [180,130, 70],\n","        11: [ 60, 20,220],\n","        12: [  0,  0,255],\n","        13: [142,  0,  0],\n","        14: [ 70,  0,  0],\n","        15: [100, 60,  0],\n","        16: [100, 80,  0],\n","        17: [230,  0,  0],\n","        18: [ 32, 11,119],\n","        19: [81,  0,  81]\n","        }\n","\n","    img_height, img_width = img.shape\n","\n","    img_color = np.zeros((img_height, img_width, 3))\n","    for row in range(img_height):\n","        for col in range(img_width):\n","            label = img[row, col] #this cycles through the whole array of predicted values and grabs the label at that point\n","\n","            img_color[row, col] = np.array(label_to_color[label]) # this slots in the colour vector for the specific label from the list above \n","\n","    return img_color\n","\n","\n","network.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n","batch_losses = []\n","for step, (imgs, label_imgs, img_ids) in enumerate(val_loader):\n","    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n","        imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n","        label_imgs = Variable(label_imgs.type(torch.LongTensor)).cuda() # (shape: (batch_size, img_h, img_w))\n","        print(\"________________________\")\n","        print(\"LABEL_IMGS.SHAPE\", label_imgs.shape)\n","\n","\n","        outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))  # imgs are the raw images from the cityscapes validation folder \n","        print(\"________________________\")\n","        print(\"OUTPUTS.SHAPE\", outputs.shape)\n","\n","        # compute the loss:\n","        loss = loss_fn(outputs, label_imgs)  # this works out cross entropy of predicted labels with gt labels (uses the weights too)\n","        loss_value = loss.data.cpu().numpy()\n","        batch_losses.append(loss_value)\n","\n","        ########################################################################\n","        # save data for visualization:\n","        ########################################################################\n","        outputs = outputs.data.cpu().numpy() # (shape: (batch_size, num_classes, img_h, img_w))\n","        print(\"________________________\")\n","        print(\"OUTPUTS:\", outputs)\n","        \n","        pred_label_imgs = np.argmax(outputs, axis=1) # (shape: (batch_size, img_h, img_w))\n","        pred_label_imgs = pred_label_imgs.astype(np.uint8)\n","        \n","        print(\"PRED_LABEL_IMGS:\", pred_label_imgs)\n","        \n","        print(pred_label_imgs[1])\n","        \n","        print(\"________________________\")\n","        print(\"PRED_LABEL_IMGS.SHAPE\", pred_label_imgs.shape)\n","        print(\"________________________\")\n","        print(\"pred_label_imgs.shape[0]\", pred_label_imgs.shape[0]) # this gives the batch size. 2 in this case \n","        print(\"________________________\")\n","        print(\"img_ids[0]\", img_ids[0])\n","        print(\"________________________\")\n","        print(\"pred_label_imgs[0]\", pred_label_imgs[0])\n","        print(\"________________________\")\n","        print(\"img_ids\", img_ids)\n","\n","        for i in range(pred_label_imgs.shape[0]): # gives batch size\n","            if i == 0: \n","            # this means we only creates the overlayed image for the first picture in the batch. Since the batch size is 2 that means we only create overlaid images for half of the val images.. probs to save time/space\n","            # notice eval_on_val_for_metrics doesnt do this as we need each image for the metrics. \n","                pred_label_img = pred_label_imgs[i] # (shape: (img_h, img_w))\n","                img_id = img_ids[i]\n","                img = imgs[i] # (shape: (3, img_h, img_w))\n","\n","                img = img.data.cpu().numpy()\n","                img = np.transpose(img, (1, 2, 0)) # (shape: (img_h, img_w, 3))\n","                img = img*np.array([0.229, 0.224, 0.225])\n","                img = img + np.array([0.485, 0.456, 0.406])\n","                img = img*255.0\n","                img = img.astype(np.uint8)\n","\n","                \n","                # convert pred_label_img from trainId to id pixel values: I added this from the eval_on_val_for_metrics script. \n","                #pred_label_img = trainId_to_id_map_func(pred_label_img) # (shape: (1024, 2048))\n","                #pred_label_img = pred_label_img.astype(np.uint8)\n","\n","                pred_label_img_color = label_img_to_color_BGR(pred_label_img) \n","                print(\"*****\")\n","                print(img)\n","                \n","                print(img_id)\n","                print(\"____________\")\n","                print(pred_label_img)\n","                print(\"____________\")\n","                print(pred_label_img_color)\n","                # pred_label_img_color_bgr = cv2.cvtColor(pred_label_img_color, cv2.COLOR_BGR2RGB) # becuase we save the image with opencv below and it has the convention BGR \n","\n","                overlayed_img = 0.35*img + 0.65*pred_label_img_color\n","                overlayed_img = overlayed_img.astype(np.uint8)\n","\n","                print(\"____________\")\n","                print(\"Network.model_dir\", network.model_dir)\n","\n","                \n","                cv2.imwrite(network.model_dir + \"/\" + img_id + \"_colour_test.png\", pred_label_img_color)\n","                cv2.imwrite(network.model_dir + \"/\" + img_id + \"_overlayed.png\", overlayed_img)\n","            break\n","        \n","    break\n","\n","\n","val_loss = np.mean(batch_losses)\n","print (\"val loss: %g\" % val_loss)\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: import-ipynb in /usr/local/lib/python3.7/dist-packages (0.1.3)\n","pretrained resnet, 18\n","num_val_batches: 250\n","________________________\n","LABEL_IMGS.SHAPE torch.Size([2, 512, 1024])\n","________________________\n","OUTPUTS.SHAPE torch.Size([2, 20, 512, 1024])\n","________________________\n","OUTPUTS: [[[[-16.40648    -16.40648    -16.40648    ... -11.184682\n","    -11.184682   -11.184682  ]\n","   [-16.40648    -16.40648    -16.40648    ... -11.184682\n","    -11.184682   -11.184682  ]\n","   [-16.40648    -16.40648    -16.40648    ... -11.184682\n","    -11.184682   -11.184682  ]\n","   ...\n","   [ -9.045122    -9.045122    -9.045122   ...  -1.3342888\n","     -1.3342888   -1.3342888 ]\n","   [ -9.045122    -9.045122    -9.045122   ...  -1.3342888\n","     -1.3342888   -1.3342888 ]\n","   [ -9.045122    -9.045122    -9.045122   ...  -1.3342888\n","     -1.3342888   -1.3342888 ]]\n","\n","  [[-10.0230875  -10.0230875  -10.0230875  ...  -8.861837\n","     -8.861837    -8.861837  ]\n","   [-10.0230875  -10.0230875  -10.0230875  ...  -8.861837\n","     -8.861837    -8.861837  ]\n","   [-10.0230875  -10.0230875  -10.0230875  ...  -8.861837\n","     -8.861837    -8.861837  ]\n","   ...\n","   [ -6.9722342   -6.9722342   -6.9722342  ...  -3.5609865\n","     -3.5609865   -3.5609865 ]\n","   [ -6.9722342   -6.9722342   -6.9722342  ...  -3.5609865\n","     -3.5609865   -3.5609865 ]\n","   [ -6.9722342   -6.9722342   -6.9722342  ...  -3.5609865\n","     -3.5609865   -3.5609865 ]]\n","\n","  [[ -0.09804866  -0.09804866  -0.09804866 ...  -1.3524932\n","     -1.3524932   -1.3524932 ]\n","   [ -0.09804866  -0.09804866  -0.09804866 ...  -1.3524932\n","     -1.3524932   -1.3524932 ]\n","   [ -0.09804866  -0.09804866  -0.09804866 ...  -1.3524932\n","     -1.3524932   -1.3524932 ]\n","   ...\n","   [ -1.7259146   -1.7259146   -1.7259146  ...  -2.2161481\n","     -2.2161481   -2.2161481 ]\n","   [ -1.7259146   -1.7259146   -1.7259146  ...  -2.2161481\n","     -2.2161481   -2.2161481 ]\n","   [ -1.7259146   -1.7259146   -1.7259146  ...  -2.2161481\n","     -2.2161481   -2.2161481 ]]\n","\n","  ...\n","\n","  [[ -8.921356    -8.921356    -8.921356   ... -10.440122\n","    -10.440122   -10.440122  ]\n","   [ -8.921356    -8.921356    -8.921356   ... -10.440122\n","    -10.440122   -10.440122  ]\n","   [ -8.921356    -8.921356    -8.921356   ... -10.440122\n","    -10.440122   -10.440122  ]\n","   ...\n","   [-13.342749   -13.342749   -13.342749   ...  -9.310539\n","     -9.310539    -9.310539  ]\n","   [-13.342749   -13.342749   -13.342749   ...  -9.310539\n","     -9.310539    -9.310539  ]\n","   [-13.342749   -13.342749   -13.342749   ...  -9.310539\n","     -9.310539    -9.310539  ]]\n","\n","  [[ -9.175376    -9.175376    -9.175376   ...  -9.604583\n","     -9.604583    -9.604583  ]\n","   [ -9.175376    -9.175376    -9.175376   ...  -9.604583\n","     -9.604583    -9.604583  ]\n","   [ -9.175376    -9.175376    -9.175376   ...  -9.604583\n","     -9.604583    -9.604583  ]\n","   ...\n","   [-10.4048195  -10.4048195  -10.4048195  ...  -7.7993946\n","     -7.7993946   -7.7993946 ]\n","   [-10.4048195  -10.4048195  -10.4048195  ...  -7.7993946\n","     -7.7993946   -7.7993946 ]\n","   [-10.4048195  -10.4048195  -10.4048195  ...  -7.7993946\n","     -7.7993946   -7.7993946 ]]\n","\n","  [[ -0.8241046   -0.8241046   -0.8241046  ...   3.056628\n","      3.056628     3.056628  ]\n","   [ -0.8241046   -0.8241046   -0.8241046  ...   3.056628\n","      3.056628     3.056628  ]\n","   [ -0.8241046   -0.8241046   -0.8241046  ...   3.056628\n","      3.056628     3.056628  ]\n","   ...\n","   [  1.8299623    1.8299623    1.8299623  ...   0.46458727\n","      0.46458727   0.46458727]\n","   [  1.8299623    1.8299623    1.8299623  ...   0.46458727\n","      0.46458727   0.46458727]\n","   [  1.8299623    1.8299623    1.8299623  ...   0.46458727\n","      0.46458727   0.46458727]]]\n","\n","\n"," [[[ -5.0474725   -5.0474725   -5.0474725  ... -10.281901\n","    -10.281901   -10.281901  ]\n","   [ -5.0474725   -5.0474725   -5.0474725  ... -10.281901\n","    -10.281901   -10.281901  ]\n","   [ -5.0474725   -5.0474725   -5.0474725  ... -10.281901\n","    -10.281901   -10.281901  ]\n","   ...\n","   [ -7.295092    -7.295092    -7.295092   ...  -3.1701348\n","     -3.1701348   -3.1701348 ]\n","   [ -7.295092    -7.295092    -7.295092   ...  -3.1701348\n","     -3.1701348   -3.1701348 ]\n","   [ -7.295092    -7.295092    -7.295092   ...  -3.1701348\n","     -3.1701348   -3.1701348 ]]\n","\n","  [[ -5.8675723   -5.8675723   -5.8675723  ...  -5.6720743\n","     -5.6720743   -5.6720743 ]\n","   [ -5.8675723   -5.8675723   -5.8675723  ...  -5.6720743\n","     -5.6720743   -5.6720743 ]\n","   [ -5.8675723   -5.8675723   -5.8675723  ...  -5.6720743\n","     -5.6720743   -5.6720743 ]\n","   ...\n","   [ -8.26113     -8.26113     -8.26113    ...  -3.9655197\n","     -3.9655197   -3.9655197 ]\n","   [ -8.26113     -8.26113     -8.26113    ...  -3.9655197\n","     -3.9655197   -3.9655197 ]\n","   [ -8.26113     -8.26113     -8.26113    ...  -3.9655197\n","     -3.9655197   -3.9655197 ]]\n","\n","  [[ -2.7142196   -2.7142196   -2.7142196  ...  -0.99230725\n","     -0.99230725  -0.99230725]\n","   [ -2.7142196   -2.7142196   -2.7142196  ...  -0.99230725\n","     -0.99230725  -0.99230725]\n","   [ -2.7142196   -2.7142196   -2.7142196  ...  -0.99230725\n","     -0.99230725  -0.99230725]\n","   ...\n","   [ -3.6002054   -3.6002054   -3.6002054  ...  -3.897986\n","     -3.897986    -3.897986  ]\n","   [ -3.6002054   -3.6002054   -3.6002054  ...  -3.897986\n","     -3.897986    -3.897986  ]\n","   [ -3.6002054   -3.6002054   -3.6002054  ...  -3.897986\n","     -3.897986    -3.897986  ]]\n","\n","  ...\n","\n","  [[ -5.9288635   -5.9288635   -5.9288635  ...  -9.002461\n","     -9.002461    -9.002461  ]\n","   [ -5.9288635   -5.9288635   -5.9288635  ...  -9.002461\n","     -9.002461    -9.002461  ]\n","   [ -5.9288635   -5.9288635   -5.9288635  ...  -9.002461\n","     -9.002461    -9.002461  ]\n","   ...\n","   [-15.613524   -15.613524   -15.613524   ... -10.486953\n","    -10.486953   -10.486953  ]\n","   [-15.613524   -15.613524   -15.613524   ... -10.486953\n","    -10.486953   -10.486953  ]\n","   [-15.613524   -15.613524   -15.613524   ... -10.486953\n","    -10.486953   -10.486953  ]]\n","\n","  [[ -5.7569184   -5.7569184   -5.7569184  ...  -6.581944\n","     -6.581944    -6.581944  ]\n","   [ -5.7569184   -5.7569184   -5.7569184  ...  -6.581944\n","     -6.581944    -6.581944  ]\n","   [ -5.7569184   -5.7569184   -5.7569184  ...  -6.581944\n","     -6.581944    -6.581944  ]\n","   ...\n","   [-13.401118   -13.401118   -13.401118   ...  -8.234092\n","     -8.234092    -8.234092  ]\n","   [-13.401118   -13.401118   -13.401118   ...  -8.234092\n","     -8.234092    -8.234092  ]\n","   [-13.401118   -13.401118   -13.401118   ...  -8.234092\n","     -8.234092    -8.234092  ]]\n","\n","  [[ -2.0233846   -2.0233846   -2.0233846  ...   1.9391738\n","      1.9391738    1.9391738 ]\n","   [ -2.0233846   -2.0233846   -2.0233846  ...   1.9391738\n","      1.9391738    1.9391738 ]\n","   [ -2.0233846   -2.0233846   -2.0233846  ...   1.9391738\n","      1.9391738    1.9391738 ]\n","   ...\n","   [  2.5761528    2.5761528    2.5761528  ...   2.2023063\n","      2.2023063    2.2023063 ]\n","   [  2.5761528    2.5761528    2.5761528  ...   2.2023063\n","      2.2023063    2.2023063 ]\n","   [  2.5761528    2.5761528    2.5761528  ...   2.2023063\n","      2.2023063    2.2023063 ]]]]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3487: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["PRED_LABEL_IMGS: [[[ 8  8  8 ... 19 19 19]\n","  [ 8  8  8 ... 19 19 19]\n","  [ 8  8  8 ... 19 19 19]\n","  ...\n","  [19 19 19 ... 19 19 19]\n","  [19 19 19 ... 19 19 19]\n","  [19 19 19 ... 19 19 19]]\n","\n"," [[ 7  7  7 ... 19 19 19]\n","  [ 7  7  7 ... 19 19 19]\n","  [ 7  7  7 ... 19 19 19]\n","  ...\n","  [19 19 19 ... 19 19 19]\n","  [19 19 19 ... 19 19 19]\n","  [19 19 19 ... 19 19 19]]]\n","[[ 7  7  7 ... 19 19 19]\n"," [ 7  7  7 ... 19 19 19]\n"," [ 7  7  7 ... 19 19 19]\n"," ...\n"," [19 19 19 ... 19 19 19]\n"," [19 19 19 ... 19 19 19]\n"," [19 19 19 ... 19 19 19]]\n","________________________\n","PRED_LABEL_IMGS.SHAPE (2, 512, 1024)\n","________________________\n","pred_label_imgs.shape[0] 2\n","________________________\n","img_ids[0] frankfurt_000001_048196\n","________________________\n","pred_label_imgs[0] [[ 8  8  8 ... 19 19 19]\n"," [ 8  8  8 ... 19 19 19]\n"," [ 8  8  8 ... 19 19 19]\n"," ...\n"," [19 19 19 ... 19 19 19]\n"," [19 19 19 ... 19 19 19]\n"," [19 19 19 ... 19 19 19]]\n","________________________\n","img_ids ('frankfurt_000001_048196', 'frankfurt_000001_055062')\n","*****\n","[[[  2  21   3]\n","  [  6  24   7]\n","  [ 11  28  11]\n","  ...\n","  [ 38  42  19]\n","  [  0   0  54]\n","  [  0   0   0]]\n","\n"," [[  0  16  17]\n","  [  0  20  19]\n","  [  4  23  22]\n","  ...\n","  [ 37  42  19]\n","  [  0   0  49]\n","  [  0   0   0]]\n","\n"," [[  9  20   3]\n","  [ 12  23   9]\n","  [ 17  26  13]\n","  ...\n","  [ 36  41  20]\n","  [  0   0  46]\n","  [  0   0   0]]\n","\n"," ...\n","\n"," [[ 77 114 106]\n","  [ 75 111 115]\n","  [ 74 110 101]\n","  ...\n","  [148 175 167]\n","  [147 174 164]\n","  [145 172 164]]\n","\n"," [[ 77 114 106]\n","  [ 75 111 115]\n","  [ 74 111 101]\n","  ...\n","  [143 172 154]\n","  [142 171 154]\n","  [142 169 154]]\n","\n"," [[ 77 115 107]\n","  [ 77 112 115]\n","  [ 75 112 102]\n","  ...\n","  [ 74  86  76]\n","  [ 71  83  72]\n","  [136 168 156]]]\n","frankfurt_000001_048196\n","____________\n","[[ 8  8  8 ... 19 19 19]\n"," [ 8  8  8 ... 19 19 19]\n"," [ 8  8  8 ... 19 19 19]\n"," ...\n"," [19 19 19 ... 19 19 19]\n"," [19 19 19 ... 19 19 19]\n"," [19 19 19 ... 19 19 19]]\n","____________\n","[[[ 35. 142. 107.]\n","  [ 35. 142. 107.]\n","  [ 35. 142. 107.]\n","  ...\n","  [ 81.   0.  81.]\n","  [ 81.   0.  81.]\n","  [ 81.   0.  81.]]\n","\n"," [[ 35. 142. 107.]\n","  [ 35. 142. 107.]\n","  [ 35. 142. 107.]\n","  ...\n","  [ 81.   0.  81.]\n","  [ 81.   0.  81.]\n","  [ 81.   0.  81.]]\n","\n"," [[ 35. 142. 107.]\n","  [ 35. 142. 107.]\n","  [ 35. 142. 107.]\n","  ...\n","  [ 81.   0.  81.]\n","  [ 81.   0.  81.]\n","  [ 81.   0.  81.]]\n","\n"," ...\n","\n"," [[ 81.   0.  81.]\n","  [ 81.   0.  81.]\n","  [ 81.   0.  81.]\n","  ...\n","  [ 81.   0.  81.]\n","  [ 81.   0.  81.]\n","  [ 81.   0.  81.]]\n","\n"," [[ 81.   0.  81.]\n","  [ 81.   0.  81.]\n","  [ 81.   0.  81.]\n","  ...\n","  [ 81.   0.  81.]\n","  [ 81.   0.  81.]\n","  [ 81.   0.  81.]]\n","\n"," [[ 81.   0.  81.]\n","  [ 81.   0.  81.]\n","  [ 81.   0.  81.]\n","  ...\n","  [ 81.   0.  81.]\n","  [ 81.   0.  81.]\n","  [ 81.   0.  81.]]]\n","____________\n","Network.model_dir /content/drive/MyDrive/Deeplabv3Flat/training_logs/model_eval_val\n","val loss: 0.4282\n"],"name":"stdout"}]}]}