{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DBvae_model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNBcyE4tOxD0tSmlYjyP4nU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"skpJFl1BeW5w"},"source":["\"\"\"\n","Here the structure of the network is made in pytorch\n","\"\"\"\n","\n","from typing import List, Union, Optional\n","import torch\n","import os\n","from logger import logger\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from scipy.stats import norm\n","\n","class Encoder(nn.Module):\n","    \"\"\"\n","    Encodes the data using a CNN\n","\n","    Input => 64x64 image\n","    Output => mean vector z_dim\n","              log_std vector z_dim\n","              predicted value\n","    \"\"\"\n","\n","    def __init__(self, z_dim: int = 50, custom_layers: Optional[nn.Sequential] = None):\n","        super().__init__()\n","\n","        self.z_dim = z_dim\n","\n","        self.layers = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=5, stride=2),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm2d(64),\n","\n","            nn.Conv2d(64, 128, kernel_size=5, stride=2),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm2d(128),\n","\n","            nn.Conv2d(128, 256, kernel_size=5, stride=2),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm2d(256),\n","\n","            nn.Conv2d(256, 512, kernel_size=5, stride=2),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm2d(512),\n","            nn.Flatten(),\n","\n","            nn.Linear(512, 1000),\n","            nn.LeakyReLU(),\n","\n","            nn.Linear(1000, z_dim*2) # get rid of z_0 term we dont need it\n","        )\n","\n","\n","    def forward(self, input: torch.Tensor):\n","        \"\"\"\n","        Perform forward pass of encoder.\n","        \"\"\"\n","\n","        out = self.layers(input)\n","\n","        # return classification, mean and log_std\n","        return out[:, 0:self.z_dim], F.softplus(out[:,self.z_dim:]) # softplus is a less harsh ReLU making sure variance vector is always positive. We dont need this constraint on mu. \n","\n","\n","class UnFlatten(nn.Module):\n","    def __init__(self, channel_size, image_size):\n","        super(UnFlatten, self).__init__()\n","        self.channel_size = channel_size\n","        self.image_size = image_size\n","\n","    def forward(self, input):\n","        return input.view(-1, self.channel_size, self.image_size, self.image_size) #-1 means torch work out that dimension given the other supplied. Similar to numpy reshape. Tensors in view share the same memory.\n","        # so this changes a 1d vector of length 'image size' to a 2d matrix of 'images size' x 'image size'. \n","        # but what it is changing is the batches? how does this work?  \n","\n","class Decoder(nn.Module):\n","    \"\"\"\n","    Encodes the data using a CNN\n","\n","    Input => sample vector z_dim\n","    Output => 64x64 image\n","\n","    4 6 13 29 61\n","    \"\"\"\n","\n","    def __init__(self, z_dim: int = 20, custom_layers: Optional[nn.Sequential] = None):\n","        super().__init__()\n","\n","        self.layers = nn.Sequential(\n","            nn.Linear(z_dim, 1000),\n","            nn.LeakyReLU(),\n","            nn.Linear(1000, 512*1*1),\n","            UnFlatten(512, 1),\n","\n","            nn.ConvTranspose2d(512, 256, kernel_size=5, stride=2),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm2d(256),\n","\n","            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm2d(128),\n","\n","            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, output_padding=1),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm2d(64),\n","\n","            nn.ConvTranspose2d(64, 3, kernel_size=5, stride=2, output_padding=1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input: torch.Tensor):\n","        \"\"\"\n","        Perform forward pass of encoder.\n","        \"\"\"\n","\n","        out = self.layers(input)\n","\n","\n","        return out\n","\n","\n","class Db_vae(nn.Module):\n","    def __init__(\n","        self,\n","        z_dim: int = 20,\n","        hist_size: int = 1000,\n","        alpha: float = 0.01,\n","        num_bins: int = 10,\n","        device: str = \"cpu\",\n","        custom_encoding_layers: Optional[nn.Sequential] = None,\n","        custom_decoding_layers: Optional[nn.Sequential] = None\n","    ):\n","        super().__init__()\n","\n","        self.device = device\n","        self.z_dim = z_dim\n","\n","        self.encoder = Encoder(z_dim, custom_encoding_layers)\n","        self.decoder = Decoder(z_dim, custom_decoding_layers)\n","\n","        self.target_dist = torch.distributions.normal.Normal(0, 1)\n","\n","        self.c1 = 1\n","        self.c2 = 1\n","        self.c3 = 0.1\n","\n","        self.num_bins = num_bins\n","        self.min_val = -15\n","        self.max_val = 15\n","        self.xlin = np.linspace(self.min_val, self.max_val, self.num_bins).reshape(1,1,self.num_bins)\n","        self.hist = np.zeros((z_dim, self.num_bins))\n","        self.means = torch.Tensor().to(self.device)\n","        self.std = torch.Tensor().to(self.device)\n","\n","        self.alpha = alpha\n","\n","    @staticmethod\n","    def init(path_to_model: str, device: str, z_dim: int):\n","        full_path_to_model = f\"results/{path_to_model}/model.pt\"\n","        if not os.path.exists(full_path_to_model):\n","            logger.error(\n","                f\"Can't find model at {full_path_to_model}\",\n","                next_step=\"Evaluation will stop\",\n","                tip=\"Double check your path to model\"\n","            )\n","            raise Exception\n","\n","        model: Db_vae = Db_vae(z_dim=z_dim, device=device)\n","\n","        try:\n","            model.load_state_dict(torch.load(full_path_to_model, map_location=device))\n","        except:\n","            logger.error(\"Unable to load model from {full_path_to_model}.\",\n","                        next_step=\"Model will not initialize\",\n","                        tip=\"Did you use the right config parameters, or custom layers from the stored model?\"\n","            )\n","\n","        logger.info(f\"Loaded model from {path_to_model}!\")\n","        return model\n","\n","\n","    def forward(self, images: torch.Tensor, labels: torch.Tensor):\n","        \"\"\"\n","        Given images, perform an encoding and decoding step and return the\n","        negative average elbo for the given batch.\n","        \"\"\"\n","        pred, mean, std = self.encoder(images)\n","\n","        loss_class = F.binary_cross_entropy_with_logits(pred, labels.float(), reduction='none')\n","\n","        # We only want to calculate the loss towards actual faces\n","        faceslicer = labels == 1\n","        facemean = mean[faceslicer]\n","        facestd = std[faceslicer]\n","\n","        # Get single samples from the distributions with reparametrisation trick\n","        dist = torch.distributions.normal.Normal(facemean, facestd)\n","        z = dist.rsample().to(self.device)\n","\n","        res = self.decoder(z)\n","\n","\n","        # calculate VAE losses\n","        loss_recon = (images[faceslicer] - res)**2\n","        loss_recon = loss_recon.view(loss_recon.shape[0],-1).mean(1)\n","\n","        loss_kl = torch.distributions.kl.kl_divergence(dist, self.target_dist)\n","        loss_kl = loss_kl.view(loss_kl.shape[0],-1).mean(1)\n","\n","        loss_vae = self.c2 * loss_recon + self.c3 * loss_kl\n","        loss_total = self.c1 * loss_class\n","\n","        # Only add loss to positions of faces, rest is zero\n","        zeros = torch.zeros(faceslicer.shape[0]).to(self.device)\n","        zeros[faceslicer] = loss_vae\n","\n","        loss_total = loss_total + zeros\n","\n","        return pred, loss_total\n","\n","    def forward_eval(self, images: torch.Tensor):\n","        \"\"\"\n","        Given images, perform an encoding and decoding step and return the\n","        negative average elbo for the given batch.\n","        \"\"\"\n","        with torch.no_grad():\n","            pred, _,_ = self.encoder(images)\n","\n","        return pred\n","\n","\n","    def interpolate(self, images: torch.Tensor, amount: int):\n","        with torch.no_grad():\n","            _, mean, std = self.encoder(images)\n","\n","            mean_1, std_1 = mean[0,:], std[0,:]\n","            mean_2, std_2 = mean[1,:], std[1,:]\n","\n","            all_mean  = torch.tensor([]).to(self.device)\n","            all_std = torch.tensor([]).to(self.device)\n","\n","            diff_mean = mean_1 - mean_2\n","            diff_std = std_1 = std_2\n","\n","            steps_mean = diff_mean / (amount-1)\n","            steps_std = diff_std / (amount-1)\n","\n","            for i in range(amount):\n","                all_mean = torch.cat((all_mean, mean_1 - steps_mean*i))\n","                all_std = torch.cat((all_std, std_1 - steps_std*i))\n","\n","            all_mean = all_mean.view(amount, -1)\n","            all_std = all_std.view(amount, -1)\n","            dist = torch.distributions.normal.Normal(all_mean, all_std)\n","            z = dist.rsample().to(self.device)\n","\n","            recon_images = self.decoder(z)\n","\n","        return recon_images\n","\n","    def build_means(self, input: torch.Tensor):\n","        _, mean, log_std = self.encoder(input)\n","\n","        self.means = torch.cat((self.means, mean))\n","\n","        return\n","\n","\n","    def build_histo(self, input: torch.Tensor):\n","        \"\"\"\n","            Creates histos or samples Qs from it\n","            NOTE:\n","            Make sure you only put faces into this\n","            functions\n","        \"\"\"\n","        _, mean, std = self.encoder(input)\n","\n","        self.means = torch.cat((self.means, mean))\n","        self.std = torch.cat((self.std, std))\n","\n","        values = norm.pdf(self.xlin, mean.unsqueeze(-1).cpu(), std.unsqueeze(-1).cpu()).sum(0)\n","        self.hist += values\n","\n","        return\n","\n","    def get_histo_max(self):\n","        probs = torch.zeros_like(self.means[:,0]).to(self.device)\n","\n","        for i in range(self.z_dim):\n","            dist = self.means[:,i].cpu().numpy()\n","\n","            hist, bins = np.histogram(dist, density=True, bins=self.num_bins)\n","\n","            bins[0] = -float('inf')\n","            bins[-1] = float('inf')\n","            bin_idx = np.digitize(dist, bins)\n","\n","            hist = hist + self.alpha\n","            hist /= np.sum(hist)\n","\n","            p = 1.0/(hist[bin_idx-1])\n","            p /= np.sum(p)\n","\n","            probs = torch.max(probs, torch.Tensor(p).to(self.device))\n","\n","        probs /= probs.sum()\n","\n","        return probs\n","\n","    def get_histo_max5(self):\n","        probs = torch.zeros_like(self.means, dtype=float).to(self.device)\n","\n","        for i in range(self.z_dim):\n","            dist = self.means[:,i].cpu().numpy()\n","\n","            hist, bins = np.histogram(dist, density=True, bins=self.num_bins)\n","\n","            bins[0] = -float('inf')\n","            bins[-1] = float('inf')\n","            bin_idx = np.digitize(dist, bins)\n","\n","            hist = hist + self.alpha\n","            hist /= np.sum(hist)\n","\n","            p = 1.0/(hist[bin_idx-1])\n","            p /= np.sum(p)\n","\n","            probs[:,i] = torch.Tensor(p).to(self.device)\n","\n","        probs = probs.sort(1, descending=True)[0][:,:5]\n","        probs = probs.prod(1)\n","\n","        print(probs)\n","        return probs\n","\n","    def get_histo_gaussian(self):\n","        \"\"\"\n","            Returns the probabilities given the means given the histo values\n","        \"\"\"\n","        results = np.empty(self.means.shape[0])\n","        hist_batch_size = 4000\n","        # Iterate in large batches over dataset to prevent memory lockup\n","        for i in range(0, self.means.shape[0], hist_batch_size):\n","            i_end = i  + hist_batch_size\n","            if i_end > self.means.shape[0]:\n","                i_end = self.means.shape[0]\n","            mean = self.means[i:i_end, :]\n","            std = self.std[i:i_end, :]\n","\n","            lins = norm.pdf(self.xlin, mean.unsqueeze(-1).cpu(), std.unsqueeze(-1).cpu())\n","            Q = lins * self.hist\n","            Q = Q.sum(-1)\n","            W = 1 / (Q + self.alpha)\n","            # Performing the max value technique, TODO: analyse top 5\n","            results[i:i_end] = W.max(-1)\n","\n","        # # Reset values\n","        self.hist.fill(0)\n","        self.means = torch.Tensor().to(self.device)\n","        self.std = torch.Tensor().to(self.device)\n","        return torch.tensor(results).to(self.device)\n","\n","    def recon_images(self, images: torch.Tensor):\n","        with torch.no_grad():\n","            pred, mean, std = self.encoder(images)\n","\n","            # Get single samples from the distributions with reparametrisation trick\n","            dist = torch.distributions.normal.Normal(mean, std)\n","            z = dist.rsample().to(self.device)\n","\n","            recon_images = self.decoder(z)\n","\n","        # return predictions and the loss\n","        return recon_images\n","\n","    def sample(self, n_samples, z_samples=[]):\n","        \"\"\"\n","        Sample n_samples from the model. Return both the sampled images\n","        (from bernoulli) and the means for these bernoullis (as these are\n","        used to plot the data manifold).\n","        \"\"\"\n","\n","        with torch.no_grad():\n","            z_samples = torch.randn(n_samples, self.z_dim).to(self.device)\n","            sampled_images = self.decoder(z_samples)\n","\n","        return sampled_images"],"execution_count":null,"outputs":[]}]}