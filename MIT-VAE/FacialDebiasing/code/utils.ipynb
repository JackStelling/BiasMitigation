{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"utils.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPsCyZ7aaYE9v6aO5vG66iL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"w53RYHccfUgh"},"source":["from typing import List\n","from logger import logger\n","from datasets.data_utils import DatasetOutput\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data.sampler import SequentialSampler\n","from torch.utils.data import ConcatDataset, DataLoader\n","import torchvision.transforms as transforms\n","from torchvision.utils import make_grid\n","import gc\n","from collections import Counter\n","from PIL import Image\n","\n","from dataset import sample_dataset, sample_idxs_from_loader, sample_idxs_from_loaders\n","\n","def calculate_accuracy(labels, pred):\n","    \"\"\"Calculates accuracy given labels and predictions.\"\"\"\n","    return float(((pred > 0) == (labels > 0)).sum()) / labels.size()[0]\n","\n","def get_best_and_worst_predictions(labels, pred, device):\n","    \"\"\"Returns indices of the best and worst predicted faces.\"\"\"\n","    n_rows = 4\n","    n_samples = n_rows**2\n","\n","    logger.info(f\"Face percentage: {float(labels.sum().item())/len(labels)}\")\n","    indices = torch.tensor([i for i in range(len(labels))]).long().to(device)\n","\n","    faceslice = labels == 1\n","    faces,       other       = pred[faceslice],    pred[~faceslice]\n","    faces_index, other_index = indices[faceslice], indices[~faceslice]\n","\n","    worst_faces = faces_index[faces.argsort()[:n_samples]]\n","    best_faces = faces_index[faces.argsort(descending=True)[:n_samples]]\n","\n","    worst_other = other_index[other.argsort(descending=True)[:n_samples]]\n","    best_other = other_index[other.argsort()[:n_samples]]\n","\n","    return best_faces, worst_faces, best_other, worst_other\n","\n","def calculate_places(name_list, setups, w, s):\n","    \"\"\"Calculates the places in the final barplot.\"\"\"\n","    x_axis = np.arange(len(setups))\n","    counter = len(name_list)-1\n","\n","    if (len(name_list) % 2) == 0:\n","        places = []\n","        times = 0\n","        while counter > 0:\n","            places.append(x_axis-(s/2)-s*times)\n","            places.append(x_axis+(s/2)+s*times)\n","\n","            times += 1\n","            counter -= 2\n","\n","    else:\n","        places = [x_axis]\n","        times = 1\n","        while counter > 0:\n","            places.append(x_axis-s*times)\n","            places.append(x_axis+s*times)\n","\n","            times += 1\n","            counter -= 2\n","\n","    return x_axis, sorted(places, key = lambda sub: (sub[0], sub[0]))\n","\n","\n","def make_bar_plot(df, name_list, setups, colors=None, training_type=None, y_label=\"\",\n","                      title=\"\", y_lim=None, y_ticks=None):\n","    \"\"\"Writes a bar plot for the final evaluation, based on the dataframe which stems from a results.csv.\"\"\"\n","    if training_type == None:\n","        training_type = name_list\n","    if colors == None:\n","        colors = np.random.rand(len(name_list), 3)\n","\n","    s = 0.8/len(name_list)\n","    w = s-0.02\n","\n","    x_axis, places = calculate_places(name_list, setups, w, s)\n","\n","\n","    _ = plt.figure(figsize=(16, 6))\n","    ax = plt.subplot(111)\n","    for i in range(len(name_list)):\n","        ax.bar(places[i], df.loc[df[\"name\"].str.contains(name_list[i]), setups].mean(), label=training_type[i],\n","                          yerr=df.loc[df[\"name\"].str.contains(name_list[i]), setups].std(),\n","                          color=colors[i], width=w, edgecolor=\"black\", linewidth=2,capsize=10)\n","\n","\n","    plt.ylabel(y_label, fontdict={\"fontsize\":20})\n","    plt.xticks(x_axis, setups, fontsize=25)\n","\n","    if y_lim != None:\n","        plt.ylim(y_lim[0], y_lim[1])\n","\n","    if y_ticks != None:\n","        plt.yticks(y_ticks, fontsize=20)\n","\n","    plt.title(title)\n","    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.10), ncol=5, frameon=False, prop={'size': 19})\n","    plt.show()\n","\n","def make_box_plot(df, name_list, training_type=None, colors=None, y_label=\"\", title=\"\", y_lim=None):\n","    \"\"\"Writes a box plot for the final evaluation, based on the dataframe which stems from a results.csv.\"\"\"\n","    if training_type == None:\n","        training_type = [\"\"] + name_list\n","\n","    fig = plt.figure(figsize=(16, 6))\n","\n","    box_plot_data=[df.loc[df[\"name\"].str.contains(name),:]['var'] for name in name_list]\n","    box = plt.boxplot(box_plot_data, patch_artist=True)\n","\n","\n","    if y_lim != None:\n","        plt.ylim(y_lim[0], y_lim[1])\n","\n","    plt.xticks(range(len(training_type)), training_type, fontsize=12)\n","\n","    if colors != None:\n","        for patch, color in zip(box['boxes'], colors):\n","            patch.set_facecolor(color)\n","\n","    plt.ylabel(y_label, fontsize=20)\n","    plt.show()\n","\n","\n","def remove_frame(plt):\n","    \"\"\"Removes frames from a pyplot plot. \"\"\"\n","    # TODO: Add annotation\n","    frame = plt.gca()\n","    for xlabel_i in frame.axes.get_xticklabels():\n","        xlabel_i.set_visible(False)\n","        xlabel_i.set_fontsize(0.0)\n","    for xlabel_i in frame.axes.get_yticklabels():\n","        xlabel_i.set_fontsize(0.0)\n","        xlabel_i.set_visible(False)\n","    for tick in frame.axes.get_xticklines():\n","        tick.set_visible(False)\n","    for tick in frame.axes.get_yticklines():\n","        tick.set_visible(False)\n","\n","def concat_batches(batch_a: DatasetOutput, batch_b: DatasetOutput):\n","    \"\"\"Concatenates two batches of data of shape image x label x idx.\"\"\"\n","    images: torch.Tensor = torch.cat((batch_a.image, batch_b.image), 0)\n","    labels: torch.Tensor = torch.cat((batch_a.label, batch_b.label), 0)\n","    idxs: torch.Tensor = torch.cat((batch_a.idx, batch_b.idx), 0)\n","\n","    return images, labels, idxs\n","\n","\n","def read_image(path_to_image):\n","    \"\"\"Reads an image into memory and transform to a tensor.\"\"\"\n","    img: Image = Image.open(path_to_image)\n","\n","    transforms = default_transforms()\n","    img_tensor: torch.Tensor = transforms(img)\n","\n","    return img_tensor\n","\n","def read_flags(path_to_model):\n","    \"\"\"\"\"\"\n","    path_to_flags = f\"results/{path_to_model}/flags.txt\"\n","\n","    with open(path_to_flags, 'r') as f:\n","        data = f.readlines()\n","\n","def find_face_in_subimages(model, sub_images: torch.Tensor, device: str):\n","    \"\"\"Finds a face in a tensor of subimages using a models' evaluation method.\"\"\"\n","    model.eval()\n","\n","    for images in sub_images:\n","        if len(images.shape) == 5:\n","            images = images.squeeze(dim=0)\n","\n","        # If one image\n","        if len(images.shape) == 3:\n","            images = images.view(1, 3, 64, 64)\n","        images = images.to(device)\n","        pred = model.forward_eval(images)\n","\n","        # If face\n","        if (pred > 0).any():\n","            return True\n","\n","    return False\n","\n","\n","def default_transforms():\n","    \"\"\"Transforms a transform object to a 64 by 64 tensor.\"\"\"\n","    return transforms.Compose([\n","        transforms.Resize((64, 64)),\n","        transforms.ToTensor()\n","    ])\n","\n","def visualize_tensor(img_tensor: torch.Tensor):\n","    \"\"\"Visualizes a image tensor.\"\"\"\n","    pil_transformer = transforms.ToPILImage()\n","    pil_transformer(img_tensor).show()"],"execution_count":null,"outputs":[]}]}