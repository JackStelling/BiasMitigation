{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"trainer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNBiObkMO+0II6CeYfsx58/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"wNYSqjvVe2_q"},"source":["import torch\n","import torch.nn as nn\n","from typing import Optional\n","from datetime import datetime\n","import os\n","from logger import logger\n","from torch.utils.data.dataset import Dataset\n","\n","from setup import init_trainining_results\n","from vae_model import Db_vae\n","from datasets.data_utils import DataLoaderTuple, DatasetOutput\n","import utils\n","from dataset import make_hist_loader, make_train_and_valid_loaders, concat_datasets, sample_dataset\n","\n","from torchvision.utils import make_grid\n","from matplotlib import pyplot as plt\n","\n","class Trainer:\n","    def __init__(\n","        self,\n","        epochs: int,\n","        batch_size: int,\n","        hist_size: int,\n","        z_dim: int,\n","        alpha: float,\n","        num_bins: int,\n","        max_images: int,\n","        debias_type: str,\n","        device: str,\n","        lr: float = 0.001,\n","        eval_freq: int = 10,\n","        optimizer = torch.optim.Adam,\n","        load_model: bool = False,\n","        run_folder: Optional[str] = None,\n","        custom_encoding_layers: Optional[nn.Sequential] = None,\n","        custom_decoding_layers: Optional[nn.Sequential] = None,\n","        path_to_model: Optional[str] = None,\n","        config: Optional = None,\n","        **kwargs\n","    ):\n","        \"\"\"Wrapper class which trains a model.\"\"\"\n","        init_trainining_results(config)\n","        self.epochs = epochs\n","        self.load_model = load_model\n","        self.z_dim = z_dim\n","        self.path_to_model = path_to_model\n","        self.batch_size = batch_size\n","        self.hist_size = hist_size\n","        self.alpha = alpha\n","        self.num_bins = num_bins\n","        self.debias_type = debias_type\n","        self.device = device\n","        self.eval_freq = eval_freq\n","        self.run_folder = run_folder\n","\n","        self.config = config\n","\n","        new_model: Db_vae = Db_vae(\n","            z_dim=z_dim,\n","            hist_size=hist_size,\n","            alpha=alpha,\n","            num_bins=num_bins,\n","            device=self.device\n","        ).to(device=self.device)\n","\n","        self.model = self.init_model()\n","\n","        self.optimizer = optimizer(params=self.model.parameters(), lr=lr)\n","\n","        train_loaders: DataLoaderTuple\n","        valid_loaders: DataLoaderTuple\n","\n","        train_loaders, valid_loaders = make_train_and_valid_loaders(\n","            batch_size=batch_size,\n","            max_images=max_images,\n","            **kwargs\n","        )\n","\n","        self.train_loaders = train_loaders\n","        self.valid_loaders = valid_loaders\n","\n","    def init_model(self):\n","        # If model is loaded from file-system\n","        if self.load_model:\n","            if self.path_to_model is None:\n","                logger.error(\n","                    \"Path has not been set.\",\n","                    next_step=\"Model will not be initialized.\",\n","                    tip=\"Set a path_to_model in your config.\"\n","                )\n","                raise Exception\n","\n","            if not os.path.exists(f\"results/{self.path_to_model}\"):\n","                logger.error(\n","                    f\"Can't find model at results/{self.path_to_model}.\",\n","                    next_step=\"Model will not be initialized.\",\n","                    tip=f\"Check if the directory results/{self.path_to_model} exists.\"\n","                )\n","                raise Exception\n","\n","            logger.info(f\"Initializing model from {self.path_to_model}\")\n","            return Db_vae.init(self.path_to_model, self.device, self.z_dim).to(self.device)\n","\n","        # Model is newly initialized\n","        logger.info(f\"Creating new model with the following parameters:\\n\"\n","                    f\"z_dim: {self.z_dim}\\n\"\n","                    f\"hist_size: {self.hist_size}\\n\"\n","                    f\"alpha: {self.alpha}\\n\"\n","                    f\"num_bins: {self.num_bins}\\n\"\n","        )\n","\n","        return Db_vae(\n","            z_dim=self.z_dim,\n","            hist_size=self.hist_size,\n","            alpha=self.alpha,\n","            num_bins=self.num_bins,\n","            device=self.device\n","        ).to(device=self.device)\n","\n","\n","    def train(self, epochs: Optional[int] = None):\n","        # Optionally use passed epochs\n","        epochs = self.epochs if epochs is None else epochs\n","\n","        # Start training and validation cycle\n","        for epoch in range(epochs):\n","            epoch_start_t = datetime.now()\n","            logger.info(f\"Starting epoch: {epoch+1}/{epochs}\")\n","\n","            self._update_sampling_histogram(epoch)\n","\n","            # Training\n","            train_loss, train_acc = self._train_epoch()\n","            epoch_train_t = datetime.now() - epoch_start_t\n","            logger.info(f\"epoch {epoch+1}/{epochs}::Training done\")\n","            logger.info(f\"epoch {epoch+1}/{epochs} => train_loss={train_loss:.2f}, train_acc={train_acc:.2f}\")\n","\n","            # Validation\n","            logger.info(\"Starting validation\")\n","            val_loss, val_acc = self._eval_epoch(epoch)\n","            epoch_val_t = datetime.now() - epoch_start_t\n","            logger.info(f\"epoch {epoch+1}/{epochs}::Validation done\")\n","            logger.info(f\"epoch {epoch+1}/{epochs} => val_loss={val_loss:.2f}, val_acc={val_acc:.2f}\")\n","\n","            # Print reconstruction\n","            valid_data = concat_datasets(self.valid_loaders.faces.dataset, self.valid_loaders.nonfaces.dataset, proportion_a=0.5)\n","            self.print_reconstruction(self.model, valid_data, epoch, self.device)\n","\n","            # Save model and scores\n","            self._save_epoch(epoch, train_loss, val_loss, train_acc, val_acc)\n","\n","        logger.success(f\"Finished training on {epochs} epochs.\")\n","\n","\n","    def print_reconstruction(self, model, data, epoch, device, n_rows=4, save=True):\n","        # TODO: Add annotation\n","        model.eval()\n","        n_samples = n_rows**2\n","\n","        images = sample_dataset(data, n_samples).to(device)\n","\n","        recon_images = model.recon_images(images)\n","\n","        fig=plt.figure(figsize=(16, 8))\n","\n","        fig.add_subplot(1, 2, 1)\n","        grid = make_grid(images.reshape(n_samples,3,64,64), n_rows)\n","        plt.imshow(grid.permute(1,2,0).cpu())\n","\n","        utils.remove_frame(plt)\n","\n","        fig.add_subplot(1, 2, 2)\n","        grid = make_grid(recon_images.reshape(n_samples,3,64,64), n_rows)\n","        plt.imshow(grid.permute(1,2,0).cpu())\n","\n","        utils.remove_frame(plt)\n","\n","        if save:\n","            fig.savefig('results/{}/reconstructions/epoch={}'.format(self.config.run_folder, epoch), bbox_inches='tight')\n","\n","            plt.close()\n","        else:\n","            return fig\n","\n","\n","    def _save_epoch(self, epoch: int, train_loss: float, val_loss: float, train_acc: float, val_acc: float):\n","        \"\"\"Writes training and validation scores to a csv, and stores a model to disk.\"\"\"\n","        if not self.run_folder:\n","            logger.warning(f\"`--run_folder` could not be found.\",\n","                           f\"The program will continue, but won't save anything\",\n","                           f\"Double-check if --run_folder is configured.\"\n","            )\n","\n","            return\n","\n","        # Write epoch metrics\n","        path_to_results = f\"results/{self.run_folder}/training_results.csv\"\n","        with open(path_to_results, \"a\") as wf:\n","            wf.write(f\"{epoch}, {train_loss}, {val_loss}, {train_acc}, {val_acc}\\n\")\n","\n","        # Write model to disk\n","        path_to_model = f\"results/{self.run_folder}/model.pt\"\n","        torch.save(self.model.state_dict(), path_to_model)\n","\n","        logger.save(f\"Stored model and results at results/{self.run_folder}\")\n","\n","    def visualize_bias(self, probs, data_loader, all_labels, all_index, epoch, n_rows=3):\n","        # TODO: Add annotation\n","        n_samples = n_rows ** 2\n","\n","        highest_probs = probs.argsort(descending=True)[:n_samples]\n","        lowest_probs = probs.argsort()[:n_samples]\n","\n","        highest_imgs = utils.sample_idxs_from_loader(all_index[highest_probs], data_loader, 1)\n","        worst_imgs = utils.sample_idxs_from_loader(all_index[lowest_probs], data_loader, 1)\n","\n","        img_list = (highest_imgs, worst_imgs)\n","        titles = (\"Highest weights\", \"Lowest weights\")\n","        fig = plt.figure(figsize=(16, 16))\n","\n","        for i in range(2):\n","            ax = fig.add_subplot(1, 2, i+1)\n","            grid = make_grid(img_list[i].reshape(n_samples,3,64,64), n_rows)\n","            plt.imshow(grid.permute(1,2,0).cpu())\n","            ax.set_title(titles[i], fontdict={\"fontsize\":30})\n","\n","            utils.remove_frame(plt)\n","\n","        path_to_results = f\"results/{self.config.run_folder}/bias_probs/epoch={epoch}\"\n","        logger.save(f\"Saving a bias probability figure in {path_to_results}\")\n","\n","        fig.savefig(path_to_results, bbox_inches='tight')\n","        plt.close()\n","\n","\n","    def _eval_epoch(self, epoch):\n","        \"\"\"Calculates the validation error of the model.\"\"\"\n","        face_loader, nonface_loader = self.valid_loaders\n","\n","        self.model.eval()\n","        avg_loss = 0\n","        avg_acc = 0\n","\n","        all_labels = torch.tensor([], dtype=torch.long).to(self.device)\n","        all_preds = torch.tensor([]).to(self.device)\n","        all_idxs = torch.tensor([], dtype=torch.long).to(self.device)\n","\n","        count = 0\n","\n","        with torch.no_grad():\n","            for i, (face_batch, nonface_batch) in enumerate(zip(face_loader, nonface_loader)):\n","                images, labels, idxs = utils.concat_batches(face_batch, nonface_batch)\n","\n","                images = images.to(self.device)\n","                labels = labels.to(self.device)\n","                idxs = idxs.to(self.device)\n","                pred, loss = self.model.forward(images, labels)\n","\n","                loss = loss.mean()\n","                acc = utils.calculate_accuracy(labels, pred)\n","\n","                avg_loss += loss.item()\n","                avg_acc += acc\n","\n","                all_labels = torch.cat((all_labels, labels))\n","                all_preds = torch.cat((all_preds, pred))\n","                all_idxs = torch.cat((all_idxs, idxs))\n","\n","                count = i\n","\n","        best_faces, worst_faces, best_other, worst_other = utils.get_best_and_worst_predictions(all_labels, all_preds, self.device)\n","        self.visualize_best_and_worst(self.valid_loaders, all_labels, all_idxs, epoch, best_faces, worst_faces, best_other, worst_other)\n","\n","        return avg_loss/(count+1), avg_acc/(count+1)\n","\n","    def _train_epoch(self):\n","        \"\"\"Trains the model for one epoch.\"\"\"\n","        face_loader, nonface_loader = self.train_loaders\n","\n","        self.model.train()\n","\n","        # The batches contain Image(rgb x w x h), Labels (1 for 0), original dataset indices\n","        face_batch: DatasetOutput\n","        nonface_batch: DatasetOutput\n","\n","        avg_loss: float = 0\n","        avg_acc: float = 0\n","        count: int = 0\n","\n","        for i, (face_batch, nonface_batch) in enumerate(zip(face_loader, nonface_loader)):\n","            images, labels, _ = utils.concat_batches(face_batch, nonface_batch)\n","            images, labels = images.to(self.device), labels.to(self.device)\n","\n","            # Forward pass\n","            pred, loss = self.model.forward(images, labels)\n","\n","            # Calculate the gradient, and clip at 5\n","            self.optimizer.zero_grad()\n","            loss = loss.mean()\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5)\n","            self.optimizer.step()\n","\n","            # Calculate metrics\n","            acc = utils.calculate_accuracy(labels, pred)\n","            avg_loss += loss.item()\n","            avg_acc += acc\n","\n","            if i % self.eval_freq == 0:\n","                logger.info(f\"Training: batch:{i} accuracy:{acc}\")\n","\n","            count = i\n","\n","        return avg_loss/(count+1), avg_acc/(count+1)\n","\n","    def _update_sampling_histogram(self, epoch: int):\n","        \"\"\"Updates the data loader for faces to be proportional to how challenge each image is, in case\n","        debias_type not none is.\n","        \"\"\"\n","        hist_loader = make_hist_loader(self.train_loaders.faces.dataset, self.batch_size)\n","\n","        if self.debias_type != 'none':\n","            hist = self._update_histogram(hist_loader, epoch)\n","            self.train_loaders.faces.sampler.weights = hist\n","        else:\n","            self.train_loaders.faces.sampler.weights = torch.ones(len(self.train_loaders.faces.sampler.weights))\n","\n","\n","    def _update_histogram(self, data_loader, epoch):\n","        \"\"\"Updates the histogram of `self.model`.\"\"\"\n","        logger.info(f\"Updating weight histogram using method: {self.debias_type}\")\n","\n","        self.model.means = torch.Tensor().to(self.device)\n","        self.model.std = torch.Tensor().to(self.device)\n","\n","        all_labels = torch.tensor([], dtype=torch.long).to(self.device)\n","        all_index = torch.tensor([], dtype=torch.long).to(self.device)\n","\n","        with torch.no_grad():\n","            for _, batch in enumerate(data_loader):\n","                images, labels, index, _ = batch\n","                images, labels, index = images.to(self.device), labels.to(self.device), index.to(self.device)\n","\n","                all_labels = torch.cat((all_labels, labels))\n","                all_index = torch.cat((all_index, index))\n","\n","                if self.debias_type == \"max\" or self.debias_type == \"max5\":\n","                    self.model.build_means(images)\n","\n","                elif self.debias_type == \"gaussian\":\n","                    self.model.build_histo(images)\n","\n","            if self.debias_type == \"max\":\n","                probs = self.model.get_histo_max()\n","            elif self.debias_type == \"max5\":\n","                probs = self.model.get_histo_max5()\n","            elif self.debias_type == \"gaussian\":\n","                probs = self.model.get_histo_gaussian()\n","            else:\n","                logger.error(\"No correct debias method given!\",\n","                            next_step=\"The program will now close\",\n","                            tip=\"Set --debias_method to 'max', 'max5' or 'gaussian'.\")\n","                raise Exception()\n","\n","        self.visualize_bias(probs, data_loader, all_labels, all_index, epoch)\n","\n","        return probs\n","\n","    def sample(self, n_rows=4):\n","        n_samples = n_rows**2\n","        sample_images = self.model.sample(n_samples = n_samples)\n","\n","        plt.figure(figsize=(n_rows*2,n_rows*2))\n","        grid = make_grid(sample_images.reshape(n_samples,3,64,64), n_rows)\n","        plt.imshow(grid.permute(1,2,0).cpu())\n","\n","        utils.remove_frame(plt)\n","        plt.show()\n","\n","        return\n","\n","    def reconstruction_samples(self, n_rows=4):\n","        valid_data = concat_datasets(self.valid_loaders.faces.dataset, self.valid_loaders.nonfaces.dataset, proportion_a=0.5)\n","        fig = self.print_reconstruction(self.model, valid_data, 0, self.device, save=False)\n","\n","        fig.show()\n","\n","        return\n","\n","\n","    def visualize_best_and_worst(self, data_loaders, all_labels, all_indices, epoch, best_faces, worst_faces, best_other, worst_other, n_rows=4, save=True):\n","        # TODO: Add annotation\n","        n_samples = n_rows**2\n","\n","        fig=plt.figure(figsize=(16, 16))\n","\n","        sub_titles = [\"Best faces\", \"Worst faces\", \"Best non-faces\", \"Worst non-faces\"]\n","        for i, indices in enumerate((best_faces, worst_faces, best_other, worst_other)):\n","            labels, indices = all_labels[indices], all_indices[indices]\n","            images = utils.sample_idxs_from_loaders(indices, data_loaders, labels[0])\n","\n","            ax = fig.add_subplot(2, 2, i+1)\n","            grid = make_grid(images.reshape(n_samples,3,64,64), n_rows)\n","            plt.imshow(grid.permute(1,2,0).cpu())\n","            ax.set_title(sub_titles[i], fontdict={\"fontsize\":30})\n","\n","            utils.remove_frame(plt)\n","\n","        if save:\n","            fig.savefig('results/{}/best_and_worst/epoch:{}'.format(self.config.run_folder, epoch), bbox_inches='tight')\n","\n","            plt.close()\n","\n","        else:\n","            return fig\n","\n","\n","    def best_and_worst(self, n_rows=4):\n","        \"\"\"Calculates the validation error of the model.\"\"\"\n","        face_loader, nonface_loader = self.valid_loaders\n","\n","        self.model.eval()\n","        avg_loss = 0\n","        avg_acc = 0\n","\n","        all_labels = torch.tensor([], dtype=torch.long).to(self.device)\n","        all_preds = torch.tensor([]).to(self.device)\n","        all_idxs = torch.tensor([], dtype=torch.long).to(self.device)\n","\n","        count = 0\n","\n","        with torch.no_grad():\n","            for i, (face_batch, nonface_batch) in enumerate(zip(face_loader, nonface_loader)):\n","                images, labels, idxs = utils.concat_batches(face_batch, nonface_batch)\n","\n","                images = images.to(self.device)\n","                labels = labels.to(self.device)\n","                idxs = idxs.to(self.device)\n","                pred, loss = self.model.forward(images, labels)\n","\n","                loss = loss.mean()\n","                acc = utils.calculate_accuracy(labels, pred)\n","\n","                avg_loss += loss.item()\n","                avg_acc += acc\n","\n","                all_labels = torch.cat((all_labels, labels))\n","                all_preds = torch.cat((all_preds, pred))\n","                all_idxs = torch.cat((all_idxs, idxs))\n","\n","                count = i\n","\n","        best_faces, worst_faces, best_other, worst_other = utils.get_best_and_worst_predictions(all_labels, all_preds, self.device)\n","        fig = self.visualize_best_and_worst(self.valid_loaders, all_labels, all_idxs, 0, best_faces, worst_faces, best_other, worst_other, save=False)\n","\n","        fig.show()\n","\n","        return"],"execution_count":null,"outputs":[]}]}